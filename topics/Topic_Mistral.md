# Mistral news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Paris-based AI startup Mistral AI raises $640M](https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/) ğŸŸ¢ | Mistral AI, a Paris-based AI startup with founders from Meta and DeepMind, secured $640M in a Series B round led by General Catalyst, reaching a $6B valuation, and focuses on creating cutting-edge AI technologies, balancing open-source and proprietary offerings. | [Funding ğŸ’°](Topic_Funding.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Meta â™¾](Topic_Meta.md), [DeepMind ğŸ§©](Topic_DeepMind.md) | 2024-06-17 |
| [Mistral releases Codestral](https://mistral.ai/news/codestral/) ğŸŸ¢ | Codestral is Mistral AIâ€™s new generative AI model focused on coding, boasting proficiency in over 80 programming languages and a large 32k context window for superior performance in benchmarks. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-06-03 |
| [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) ğŸŸ¢ | Mistral has launched version 3 of their 7B model, the models â€œMistral-7B-v0.3â€ and â€œMistral-7B-Instruct-v0.3â€. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-05-27 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) ğŸŸ¢ | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-04-23 |
| [Mistral AI releases new model to rival GPT-4 and its own chat assistant](https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/) ğŸŸ¢ | Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies. | [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-04 |
| [Microsoft partners with Mistral in second AI deal beyond OpenAI](https://www.theverge.com/2024/2/26/24083510/microsoft-mistral-partnership-deal-azure-ai) ğŸŸ¢ | Microsoft enters a multiyear collaboration with Mistral, valued at â‚¬2 billion, acquiring a minor stake shortly after its significant investment in OpenAI. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-03-04 |
| [Mistral Confirms New Open Source AI Model Nearing GPT-4 Performance](https://news.slashdot.org/story/24/01/31/2145205/mistral-confirms-new-open-source-ai-model-nearing-gpt-4-performance) ğŸŸ¢ | Mistral has recently confirmed that the â€œmiqu-1â€“70bâ€ Large Language Model, released on HuggingFace and exhibiting performance close to that of GPT-4, is a leaked quantized version of their technology. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [AI and copyright Â©ï¸](Topic_AI_and_copyright.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-02-05 |
| [Hugging Face launches open source AI assistant maker to rival OpenAIâ€™s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) ğŸŸ¢ | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAIâ€™s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistralâ€™s Mixtral and Metaâ€™s Llama 2. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-02-05 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) ğŸŸ¢ | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistralâ€™s beta API and dev platform. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-11 |
| [Mistral is in discussions to raise a major round at a valuation of at least $2 billion](https://www.businessinsider.com/mistral-in-talks-to-raise-funding-at-2-billion-valuation-2023-11) ğŸŸ¢ | Mistral, an AI startup aiming to rival OpenAI, is in talks for a significant funding round led by Andreessen Horowitz and General Catalyst. This round could value the company at over $2 billion and propel it to unicorn status in just six months. Mistralâ€™s open-source models have gained attention, fueling excitement for generative AI, and investors are eagerly vying for the opportunity to support the â€œnext OpenAIâ€. | [Funding ğŸ’°](Topic_Funding.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-12-11 |
| [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) ğŸŸ¢ | The Mistral 7B model, powered by Grouped-query attention (GQA) and Sliding Window Attention (SWA), outperforms other models in various domains while maintaining strong performance in both English and coding tasks. Its impressive benchmarks make it the best 7B model to date, enhancing inference speed and sequence handling efficiency. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md) | 2023-10-02 |
| [Introducing 100K Context Windows: Claude, by Anthropic](https://www.anthropic.com/index/100k-context-windows) ğŸŸ¢ | AI language model, Claude, now analyzes and synthesizes vast text in seconds with a 100K token context window. Summarize docs, assess risks, and more. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Anthropic ğŸŒ](Topic_Anthropic.md) | 2023-05-16 |