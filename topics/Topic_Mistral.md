# Mistral news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Mistral releases Pixtral Large, a 124B multimodal model](https://mistral.ai/news/pixtral-large/) ğŸŸ¢ | Mistral AI unveils Pixtral Large, a 124B multimodal model excelling in image and text understanding. Built on Mistral Large 2, it surpasses models like GPT-4o on benchmarks such as MathVista and DocVQA. Available for both research and commercial use, it boasts a 128K context window and advanced capabilities. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-11-25 |
| [Mistral releases new AI models optimized for laptops and phones](https://techcrunch.com/2024/10/16/mistral-releases-new-ai-models-optimized-for-edge-devices/) ğŸŸ¢ | Mistral has introduced â€œLes Ministraux,â€ a series of AI models, Ministral 3B and Ministral 8B, optimized for edge devices like laptops and phones. These models focus on privacy-first, low-latency applications such as on-device translation and local analytics. Available for download or via Mistralâ€™s cloud platform, they reportedly outperform competitors in AI benchmarks. This launch follows Mistralâ€™s recent $640 million funding round, signaling continued expansion in AI offerings. | [Model release ğŸ‰](Topic_Model_release.md), [Funding ğŸ’°](Topic_Funding.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-10-21 |
| [Zyphra releases Zamba2â€“7B](https://zyphra.webflow.io/post/zamba2-7b) ğŸŸ¢ | Zyphra has introduced Zamba2â€“7B, a 7B-scale language model that outperforms competitors such as Mistral, Googleâ€™s Gemma, and Metaâ€™s Llama3. It features innovative Mamba2 blocks and dual shared attention layers, resulting in improved inference speed and reduced memory usage. | [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md), [Google ğŸ”](Topic_Google.md) | 2024-10-21 |
| [Mistral releases Pixtral 12B, its first multimodal model](https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/) ğŸŸ¢ | Mistral has introduced Pixtral 12B, a 12-billion-parameter multimodal AI model that processes both text and images. Building on features from their previous text model, Nemo 12B, Pixtral 12B is available for free download on GitHub and Hugging Face under an Apache 2.0 license. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-09-16 |
| [Mistral alpha release of agents](https://mistral.ai/news/build-tweak-repeat/) ğŸŸ¢ | Mistral has introduced customization options for its models, including base prompts, few-shot prompting, and fine-tuning. The platform also launched an alpha version of Agents for workflow automation and debuted a stable client SDK for improved integration and application development. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-08-12 |
| [Mistral releases Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) ğŸŸ¢ | Mistral launched their new model, Mistral Large 2, with 123 billion parameters and a 128k context window, offering multi-language and programming language support, optimized for high-throughput single-node inference. It delivers 84.0% accuracy on the MMLU benchmark, exhibits enhanced code generation, and reasoning capabilities. The model is available with research and commercial licensing options. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-07-29 |
| [Mistral NeMo](https://mistral.ai/news/mistral-nemo/) ğŸŸ¢ | Mistral, in collaboration with NVIDIA, has launched the 12B parameter Mistral NeMo model, featuring a 128k token context window, FP8 inference compatibility, and a cutting-edge Tekken tokenizer. It is open-sourced under Apache 2.0, boasts enhanced multilingual capabilities, and outperforms the previous 7B version in instruction-following tasks. | [Model release ğŸ‰](Topic_Model_release.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-07-22 |
| [Codestral Mamba](https://mistral.ai/news/codestral-mamba/) ğŸŸ¢ | Mistral has introduced Codestral Mamba, a new coding-centric Mamba model known for efficiently managing long sequences with linear time inference and theoretical support for unlimited sequence lengths. It competes with leading SOTA models and is open-source, accessible for extension through the GitHub repository with integration options like mistral-inference SDK, TensorRT-LLM, and an upcoming llama.cpp. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-07-22 |
| [Paris-based AI startup Mistral AI raises $640M](https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/) ğŸŸ¢ | Mistral AI, a Paris-based AI startup with founders from Meta and DeepMind, secured $640M in a Series B round led by General Catalyst, reaching a $6B valuation, and focuses on creating cutting-edge AI technologies, balancing open-source and proprietary offerings. | [Funding ğŸ’°](Topic_Funding.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Meta â™¾](Topic_Meta.md), [DeepMind ğŸ§©](Topic_DeepMind.md) | 2024-06-17 |
| [Mistral releases Codestral](https://mistral.ai/news/codestral/) ğŸŸ¢ | Codestral is Mistral AIâ€™s new generative AI model focused on coding, boasting proficiency in over 80 programming languages and a large 32k context window for superior performance in benchmarks. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-06-03 |
| [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) ğŸŸ¢ | Mistral has launched version 3 of their 7B model, the models â€œMistral-7B-v0.3â€ and â€œMistral-7B-Instruct-v0.3â€. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-05-27 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) ğŸŸ¢ | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-04-23 |
| [Mistral AI releases new model to rival GPT-4 and its own chat assistant](https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/) ğŸŸ¢ | Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies. | [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-04 |
| [Microsoft partners with Mistral in second AI deal beyond OpenAI](https://www.theverge.com/2024/2/26/24083510/microsoft-mistral-partnership-deal-azure-ai) ğŸŸ¢ | Microsoft enters a multiyear collaboration with Mistral, valued at â‚¬2 billion, acquiring a minor stake shortly after its significant investment in OpenAI. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-03-04 |
| [Mistral Confirms New Open Source AI Model Nearing GPT-4 Performance](https://news.slashdot.org/story/24/01/31/2145205/mistral-confirms-new-open-source-ai-model-nearing-gpt-4-performance) ğŸŸ¢ | Mistral has recently confirmed that the â€œmiqu-1â€“70bâ€ Large Language Model, released on HuggingFace and exhibiting performance close to that of GPT-4, is a leaked quantized version of their technology. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [AI and copyright Â©ï¸](Topic_AI_and_copyright.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-02-05 |
| [Hugging Face launches open source AI assistant maker to rival OpenAIâ€™s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) ğŸŸ¢ | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAIâ€™s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistralâ€™s Mixtral and Metaâ€™s Llama 2. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-02-05 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) ğŸŸ¢ | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistralâ€™s beta API and dev platform. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-11 |
| [Mistral is in discussions to raise a major round at a valuation of at least $2 billion](https://www.businessinsider.com/mistral-in-talks-to-raise-funding-at-2-billion-valuation-2023-11) ğŸŸ¢ | Mistral, an AI startup aiming to rival OpenAI, is in talks for a significant funding round led by Andreessen Horowitz and General Catalyst. This round could value the company at over $2 billion and propel it to unicorn status in just six months. Mistralâ€™s open-source models have gained attention, fueling excitement for generative AI, and investors are eagerly vying for the opportunity to support the â€œnext OpenAIâ€. | [Funding ğŸ’°](Topic_Funding.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-12-11 |
| [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) ğŸŸ¢ | The Mistral 7B model, powered by Grouped-query attention (GQA) and Sliding Window Attention (SWA), outperforms other models in various domains while maintaining strong performance in both English and coding tasks. Its impressive benchmarks make it the best 7B model to date, enhancing inference speed and sequence handling efficiency. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md) | 2023-10-02 |
| [Introducing 100K Context Windows: Claude, by Anthropic](https://www.anthropic.com/index/100k-context-windows) ğŸŸ¢ | AI language model, Claude, now analyzes and synthesizes vast text in seconds with a 100K token context window. Summarize docs, assess risks, and more. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Anthropic ğŸŒ](Topic_Anthropic.md) | 2023-05-16 |