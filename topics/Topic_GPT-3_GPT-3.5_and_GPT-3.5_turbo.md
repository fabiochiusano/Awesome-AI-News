# GPT-3, GPT-3.5, and GPT-3.5 turbo news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) ğŸŸ¢ | Databricks has unveiled DBRX, a new open-source large language model (LLM) that surpasses GPT-3.5 in programming and general tasks, and is competitive with Gemini 1.0 Pro. DBRX features a mixture-of-experts architecture with a massive 132 billion parameters, though only 36 billion are active for any given input. Both DBRX Base and DBRX Instruct models are openly accessible on Hugging Face. | [Hugging Face ğŸ¤—](topics/Topic_Hugging_Face_ğŸ¤—.md), [Mixture-of-Experts ğŸ¤](topics/Topic_Mixture-of-Experts_ğŸ¤.md), [AI for coding ğŸ‘¨â€ğŸ’»](topics/Topic_AI_for_coding_ğŸ‘¨â€ğŸ’».md), [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2024-04-02 |
| [OpenAI releases new embedding models and API updates](https://openai.com/blog/new-embedding-models-and-api-updates) ğŸŸ¢ | OpenAI has announced updates to their AI model suite, including the launch of more efficient embedding models and cost-reduced versions of GPT-3.5 Turbo and a new GPT-4 Turbo model. The â€œtext-embedding-3-largeâ€ leads with a 64.6% MTEB score at $0.00013 per 1k tokens, while the â€œtext-embedding-3-smallâ€ offers improved performance over its predecessor at a fivefold cost reduction. Additionally, the â€œgpt-3.5-turbo-0125â€ is now 50% cheaper, priced at $0.0005 per 1k tokens, and a new â€œgpt-4â€“0125-previewâ€ model has been introduced. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [GPT-4 and GPT-4 turbo ğŸš€](topics/Topic_GPT-4_and_GPT-4_turbo_ğŸš€.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2024-01-29 |
| [Introducing gigaGPT: GPT-3 sized models in 565 lines of code](https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code) ğŸŸ¢ | Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [AI Chips and GPUs ğŸ–¥ï¸](topics/Topic_AI_Chips_and_GPUs_ğŸ–¥ï¸.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-12-19 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) ğŸŸ¢ | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistralâ€™s beta API and dev platform. | [Mixture-of-Experts ğŸ¤](topics/Topic_Mixture-of-Experts_ğŸ¤.md), [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [Mistral ğŸŒ¬ï¸](topics/Topic_Mistral_ğŸŒ¬ï¸.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-12-11 |
| [Announcements from OpenAI DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday) ğŸŸ¢ | OpenAI has introduced several new and improved models and APIs, including GPT-4 Turbo with a larger context window and lower prices, the ability to process images in the Chat Completions API, fine- tuning options for GPT-4 and GPT-3.5 Turbo, and the availability of DALLÂ·E 3 via API. They have also introduced features like JSON mode, improved instruction following, and parallel function calling. Additionally, there are new options for text-to-speech and the creation of â€œGPT assistants.â€ OpenAI has also released the Whisper large-v3 model for automatic speech recognition. | [Whisper ğŸ¤«](topics/Topic_Whisper_ğŸ¤«.md), [Text-to-speech ğŸ“¢](topics/Topic_Text-to-speech_ğŸ“¢.md), [Speech-to-text ğŸ¤](topics/Topic_Speech-to-text_ğŸ¤.md), [AI for images ğŸ–¼ï¸](topics/Topic_AI_for_images_ğŸ–¼ï¸.md), [Multimodal AI (image, video, audio) ğŸ“¸](topics/Topic_Multimodal_AI_(image_video_audio)_ğŸ“¸.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [GPT-4 and GPT-4 turbo ğŸš€](topics/Topic_GPT-4_and_GPT-4_turbo_ğŸš€.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-11-13 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) ğŸŸ¢ | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its modelâ€™s knowledge base. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [Meta â™¾](topics/Topic_Meta_â™¾.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-10-10 |
| [Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b) ğŸŸ¢ | TII has just released Falcon 180B, a powerful language model with 180 billion parameters trained on 3.5 trillion tokens. Outperforming Llama 2 70B and GPT-3.5 on MMLU, Falcon 180B performs great and ranks high on the Hugging Face Leaderboard. This model is available for commercial use but has strict terms excluding â€œhosting use.â€ | [Hugging Face ğŸ¤—](topics/Topic_Hugging_Face_ğŸ¤—.md), [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-09-11 |
| [GPT-3.5 Turbo fine-tuning released](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates) ğŸŸ¢ | OpenAI has released fine-tuning for GPT-3.5 Turbo, offering improved performance on specific tasks. The fine-tuned version can even match or surpass the capabilities of base GPT-4. Early testers have significantly reduced prompt size through fine-tuning. The costs for training and usage input/output are provided at $0.008, $0.012, and $0.016 per 1K tokens, respectively. | [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [GPT-4 and GPT-4 turbo ğŸš€](topics/Topic_GPT-4_and_GPT-4_turbo_ğŸš€.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-08-28 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) ğŸŸ¢ | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [Stability AI âš–ï¸](topics/Topic_Stability_AI_âš–ï¸.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-07-31 |
| [GPT-4 API general availability and deprecation of older models in the Completions API](https://openai.com/blog/gpt-4-api-general-availability) ğŸŸ¢ | OpenAI has announced the general availability of the GPT-4 API and the automatic upgrade of GPT-3 models to new models from January 4, 2024. Developers using text-davinci-003 are advised to upgrade to gpt-3.5-turbo-instruct and specify it as the â€œmodelâ€ in API requests for a smooth transition. OpenAI also offers priority access to GPT-3.5 Turbo and GPT-4 fine-tuning for users with fine-tuned older models, understanding the challenges of transitioning from these models. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [GPT-4 and GPT-4 turbo ğŸš€](topics/Topic_GPT-4_and_GPT-4_turbo_ğŸš€.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-07-10 |
| [Function calling and other API updates](https://openai.com/blog/function-calling-and-other-api-updates) ğŸŸ¢ | OpenAI has released several new updates, including a 16k context version of GPT-3.5-turbo, a new API for calling user-defined functions, cost reductions for models and input tokens, and updated versions of GPT-4 and GPT-3.5-turbo. The updates aim to improve GPTâ€™s capabilities and make it easier to connect with external tools and APIs. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [GPT-4 and GPT-4 turbo ğŸš€](topics/Topic_GPT-4_and_GPT-4_turbo_ğŸš€.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-06-20 |
| [Introducing ChatGPT and Whisper APIs](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) ğŸŸ¢ | The ChatGPT API is x10 cheaper than GPT3 API. | [ChatGPT ğŸ’¬](topics/Topic_ChatGPT_ğŸ’¬.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-03-06 |
| [OpenAI Foundry will let customers buy dedicated compute to run GPT3 and their other models](https://techcrunch.com/2023/02/21/openai-foundry-will-let-customers-buy-dedicated-capacity-to-run-its-ai-models/) ğŸŸ¢ | It will soon be possible to have dedicated GPT3 models. | [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-02-27 |
| [A collection of 600+ apps powered by GPT3](https://gpt3demo.com/map) ğŸŸ¢ | and organized by category. | [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-02-20 |
| [Chatbots with GPT3](https://medium.com/nlplanet/building-chatbots-with-gpt3-62f6567d8fa4) ğŸŸ¢ | Recent advancements in LLMs, such as GPT-3, can be used for chatbot development. Instead of having many very specific intents, each intent can be broader and leverage a Knowledge Base document. | [ChatGPT ğŸ’¬](topics/Topic_ChatGPT_ğŸ’¬.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-02-06 |
| [Querying NBA stats with GPT-3 + Statmuse + Langchain](https://www.geoffreylitt.com/2023/01/29/fun-with-compositional-llms-querying-basketball-stats-with-gpt-3-statmuse-langchain.html) ğŸŸ¢ | Using Langchain, the author composed an AI program that combines GPT-3 with Statmuse, a sports stats search engine, to answer multi-part questions about NBA stats. | [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-02-06 |
| [GPT-3 integrated in Microsoft Teams Premium](https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/) ğŸŸ¢ | with AI-generated notes and tasks. | [Microsoft ğŸªŸ](topics/Topic_Microsoft_ğŸªŸ.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-02-06 |
| [Transformer models: an introduction and catalog â€” 2023 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/) ğŸŸ¢ | A catalog of popular transformer models from the last years, including ChatGPT, GPT3.5, and other models from Eleuther, Anthropic, Deepmind, and Stability. | [Anthropic ğŸŒ](topics/Topic_Anthropic_ğŸŒ.md), [DeepMind ğŸ§©](topics/Topic_DeepMind_ğŸ§©.md), [ChatGPT ğŸ’¬](topics/Topic_ChatGPT_ğŸ’¬.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2023-01-23 |