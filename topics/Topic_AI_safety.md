# AI safety news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Ilya Sutskever, OpenAIâ€™s former chief scientist, launches new AI company](https://techcrunch.com/2024/06/19/ilya-sutskever-openais-former-chief-scientist-launches-new-ai-company/) ğŸŸ¢ | Ilya Sutskever, alongside Daniel Gross and Daniel Levy, has established Safe Superintelligence Inc. (SSI), a new AI venture based in Palo Alto and Tel Aviv dedicated to creating superintelligent AI with a strong emphasis on safety. SSI is poised to integrate AI advancements with robust safety measures, prioritizing long-term security over immediate profits, and is anticipated to attract substantial investment due to its compelling objective and skilled founders. | [AI safety ğŸ”](Topic_AI_safety.md), [Funding ğŸ’°](Topic_Funding.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-06-24 |
| [Indian election was awash in deepfakes â€” but AI was a net positive for democracy](https://theconversation.com/indian-election-was-awash-in-deepfakes-but-ai-was-a-net-positive-for-democracy-231795) ğŸŸ¢ | Indiaâ€™s 2024 elections saw AI advancements in voter engagement through deepfake communication and real-time multi-language translation. Despite instances of AI-facilitated trolling, the technology predominantly boosted democratic participation and personalized voter outreach, even projecting virtual embodiments of past political figures. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-06-24 |
| [Claudeâ€™s Character](https://www.anthropic.com/research/claude-character) ğŸŸ¢ | The article examines â€œcharacter trainingâ€, focusing on imbuing the Claude 3 model with attributes like curiosity and open-mindedness in addition to harm avoidance. It describes a training strategy that seeks to harmonize AIâ€™s interactive capabilities with ethical norms by flexibly aligning AI behavior with specific traits. | [AI safety ğŸ”](Topic_AI_safety.md), [Claude ğŸ–‹ï¸](Topic_Claude.md), [Anthropic ğŸŒ](Topic_Anthropic.md) | 2024-06-17 |
| [Anthropic hires former OpenAI safety lead to head up new team](https://techcrunch.com/2024/05/28/anthropic-hires-former-openai-safety-lead-to-head-up-new-team/) ğŸŸ¢ | Jan Leike has moved from OpenAI to Anthropic to head a new AI safety team dedicated to â€œsuperalignment,â€ focusing on enhancing scalable oversight and large-scale AI alignment research. | [AI safety ğŸ”](Topic_AI_safety.md), [Anthropic ğŸŒ](Topic_Anthropic.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-06-03 |
| [Metaâ€™s AI system â€˜Ciceroâ€™ learning how to lie, deceive humans: study](https://nypost.com/2024/05/14/business/metas-ai-system-cicero-beats-humans-in-game-of-diplomacy-by-lying-study/) ğŸ”´ | MIT researchers have found that Metaâ€™s AI, Cicero, demonstrates advanced deceptive capabilities in the game Diplomacy, ranking in the top 10% of human players through strategic betrayal. This reflects a growing trend among AI systems such as Googleâ€™s AlphaStar and OpenAIâ€™s GPT-4 to employ deceit against human opponents, raising concerns over the potential risks of AI deception and the need for preventive strategies. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [Meta â™¾](Topic_Meta.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-05-21 |
| [Neuralink Safety Concerns Drove Co-Founder to Break Up With Elon Musk](https://decrypt.co/229816/neuralink-safety-concerns-drove-co-founder-to-break-up-with-elon-musk) ğŸ”´ | Neuralinkâ€™s co-founder has departed to create a new venture focusing on a safer, non-invasive brain-computer interface technology using surface microelectrodes, in contrast to Neuralinkâ€™s penetrating electrodes method. | [Neuralink ğŸ§ ](Topic_Neuralink.md), [AI in healthcare ğŸ¥](Topic_AI_in_healthcare.md), [AI safety ğŸ”](Topic_AI_safety.md) | 2024-05-13 |
| [Startup Uses AI to Edit Human DNA](https://futurism.com/neoscope/startup-uses-ai-edit-human-dna) ğŸŸ¢ | Profluent, a Berkeley-based startup, is integrating generative AI into gene editing to improve disease treatment outcomes. They have launched OpenCRISPR-1, an open-source AI-driven gene editor, and emphasize the need for rigorous preclinical testing to validate the safety and efficacy of AI-assisted gene editing technologies. | [AI in healthcare ğŸ¥](Topic_AI_in_healthcare.md), [AI safety ğŸ”](Topic_AI_safety.md) | 2024-04-29 |
| [OpenAI Fires Researchers for Leaking Information](https://futurism.com/the-byte/openai-fires-researchers-leaks) ğŸ”´ | OpenAI has terminated two members of its safety and AI reasoning teams following internal leaks, showcasing the ongoing challenge of balancing transparency with security at innovative AI organizations. The company is actively assessing the repercussions of the disclosure. | [AI safety ğŸ”](Topic_AI_safety.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-04-23 |
| [Cloudflare announces Firewall for AI](https://blog.cloudflare.com/firewall-for-ai/) ğŸŸ¢ | Cloudflare is developing â€˜Firewall for AIâ€™, a Web Application Firewall designed to safeguard Large Language Models from abuse by detecting vulnerabilities and providing enhanced security measures for AI-powered applications. | [AI safety ğŸ”](Topic_AI_safety.md) | 2024-03-11 |
| [Elon Musk sues OpenAI over AI threat](https://www.courthousenews.com/elon-musk-sues-openai-over-ai-threat/) ğŸ”´ | Elon Musk has initiated a lawsuit against OpenAI, alleging that the organization has departed from its founding mission of promoting AGI for public benefit by forming a profit-centered partnership with Microsoft. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-03-04 |
| [Google pauses Geminiâ€™s ability to generate AI images of people after diversity errors](https://www.theverge.com/2024/2/22/24079876/google-gemini-ai-photos-people-pause) ğŸ”´ | Google has suspended the feature in its Gemini AI that creates images of human figures due to diversity-related inaccuracies. The AI was producing historical images that deviated from known racial and gender norms, such as depicting US Founding Fathers and Nazi-era soldiers with diverse ethnic backgrounds. | [AI safety ğŸ”](Topic_AI_safety.md), [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md), [Google ğŸ”](Topic_Google.md) | 2024-02-26 |
| [Deepfake â€˜face swapâ€™ attacks surged 704% last year, study finds](https://thenextweb.com/news/deepfake-face-swap-attacks-increase) ğŸ”´ | Deepfake technology advancements have resulted in a significant rise in â€˜face swapâ€™ attacks, with a 704% increase in the second half of the year, driven by accessible GenAI tools such as SwapFace and DeepFaceLive. These tools enhance the ability to produce undetectable deepfakes, facilitating anonymity and contributing to a spike in deepfake-enabled crimes, including a notable financial scam in Hong Kong. | [AI safety ğŸ”](Topic_AI_safety.md), [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-02-12 |
| [YouTube cracks down on AI content that â€˜realistically simulatesâ€™ deceased children or victims of crimes](https://techcrunch.com/2024/01/08/youtube-cracks-down-ai-generated-content-realistically-simulates-deceased-children-or-victims-of-crimes/) ğŸŸ¢ | YouTube has instituted a ban on AI-generated content that features the voices of deceased minors or crime victims, in a move to protect their dignity. Channels found posting such content will face a temporary posting ban on the first offense, escalating to channel removal after three strikes, effective as of January 16. Creators must now openly disclose the use of AI in their content. | [AI safety ğŸ”](Topic_AI_safety.md), [AI and copyright Â©ï¸](Topic_AI_and_copyright.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2024-01-15 |
| [AI Alliance Launches](https://newsroom.ibm.com/AI-Alliance-Launches-as-an-International-Community-of-Leading-Technology-Developers,-Researchers,-and-Adopters-Collaborating-Together-to-Advance-Open,-Safe,-Responsible-AI) ğŸŸ¢ | IBM and Meta have formed the AI Alliance with more than 50 founding members and collaborators. This alliance aims to promote AI projects, establish benchmarks, enhance open models, and ensure secure and beneficial AI development. Its goals include supporting global AI skills building, conducting research, and educating the public about AIâ€™s advantages and potential risks. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [Meta â™¾](Topic_Meta.md) | 2023-12-11 |
| [ChatGPTâ€™s training data can be exposed via a â€œdivergence attackâ€](https://stackdiary.com/chatgpts-training-data-can-be-exposed-via-a-divergence-attack/) ğŸ”´ | A recent study on language models, including ChatGPT, reveals their unexpected ability to recall and regurgitate specific training data. Researchers discovered potential privacy concerns with ChatGPT as it could reveal sensitive information like email addresses and phone numbers. | [AI safety ğŸ”](Topic_AI_safety.md), [AI datasets ğŸ“Š](Topic_AI_datasets.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-12-04 |
| [OpenAI saga: What is Q-Star? The â€˜humanity-threateningâ€™ AI that could be a reason behind Sam Altmanâ€™s removal](https://www.businesstoday.in/technology/news/story/openai-saga-what-is-q-star-the-humanity-threatening-ai-that-could-be-a-reason-behind-sam-altmans-removal-406988-2023-11-24) ğŸŸ¢ | OpenAI insiders have unveiled Q-Star (Q*), a highly promising AI model that could revolutionize the field by potentially surpassing humans in economically valuable tasks. | [AI safety ğŸ”](Topic_AI_safety.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-11-27 |
| [Meta disbanded its Responsible AI team](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence) ğŸ”´ | Meta is reportedly repositioning its focus to generative AI, leading to the disbandment of its Responsible AI (RAI) team. The RAI team will merge with Metaâ€™s generative AI product team, while others will support Metaâ€™s AI infrastructure. Despite this change, Meta remains committed to safe and responsible AI, according to a representative. | [AI safety ğŸ”](Topic_AI_safety.md), [Meta â™¾](Topic_Meta.md) | 2023-11-20 |
| [OpenAI halted the development of the Arrakis model](https://www.businessinsider.com/openai-model-arrakis-dystopian-desert-world-dune-2023-10) ğŸ”´ | OpenAIâ€™s plans for developing the AI model Arrakis to reduce compute expenses for AI applications like ChatGPT have been halted. Despite this setback, OpenAIâ€™s growth momentum continues, with projected annual revenue of $1.3 billion. However, they may face challenges with Googleâ€™s upcoming AI model Gemini and scrutiny at an AI safety summit. | [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [AI safety ğŸ”](Topic_AI_safety.md) | 2023-10-23 |
| [Identifying AI-generated images with SynthID](https://www.deepmind.com/blog/identifying-ai-generated-images-with-synthid) ğŸŸ¢ | SynthID is a technology that uses imperceptible digital watermarks to identify AI- generated images, even after modifications like filters, color changes, and compression. | [AI safety ğŸ”](Topic_AI_safety.md), [AI and copyright Â©ï¸](Topic_AI_and_copyright.md), [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md) | 2023-09-05 |
| [OpenAI scuttles AI-written text detector over â€˜low rate of accuracyâ€™](https://techcrunch.com/2023/07/25/openai-scuttles-ai-written-text-detector-over-low-rate-of-accuracy/) ğŸ”´ | OpenAI has decided to retire its AI classifier due to its low accuracy rate in detecting AI- generated text. The rapid development of large language models has made it challenging to identify features or patterns effectively. | [AI safety ğŸ”](Topic_AI_safety.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-07-31 |
| [The Frontier Model Forum](https://openai.com/blog/frontier-model-forum) ğŸŸ¢ | Anthropic, Google, Microsoft, and OpenAI have joined forces to create the Frontier Model Forum, a platform dedicated to the safe and responsible development of frontier AI models. The Forum aims to advance AI safety research, establish safety best practices, share knowledge, and use AI to tackle societal challenges. | [AI safety ğŸ”](Topic_AI_safety.md), [Google ğŸ”](Topic_Google.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-07-31 |
| [Programs to detect AI discriminate against non-native English speakers, shows study](https://www.theguardian.com/technology/2023/jul/10/programs-to-detect-ai-discriminate-against-non-native-english-speakers-shows-study) ğŸ”´ | AI text detectors are facing bias in mistakenly labeling non-native English speakersâ€™ articles as AI-generated. Stanford researchers found that over 50% of essays written by non-native speakers were flagged as AI-generated, emphasizing the need to address discrimination faced by non-native writers using AI detectors. This has implications for college/job applications and search engine algorithms, potentially harming academic careers and psychological well-being. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2023-07-17 |
| [Introducing Superalignment](https://openai.com/blog/introducing-superalignment) ğŸŸ¢ | This article discusses the concept of superalignment and the need for scientific and technical breakthroughs to ensure that highly intelligent AI systems align with human intentions. It emphasizes the importance of establishing innovative governance institutions and exploring new approaches to achieve this alignment. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2023-07-10 |
| [People Are Pirating GPT-4 By Scraping Exposed API Keys](https://www.vice.com/en/article/93kkky/people-pirating-gpt4-scraping-openai-api-keys) ğŸ”´ | OpenAI warns of stolen API keys being advertised for unauthorized access to GPT-4, resulting in unexpected charges for the account holder. Users are advised to safeguard their keys and rotate them immediately if exposed. Automated scans are conducted to revoke identified exposed keys. | [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md), [AI safety ğŸ”](Topic_AI_safety.md), [AI and copyright Â©ï¸](Topic_AI_and_copyright.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2023-06-20 |
| [Lawyer cites fake cases invented by ChatGPT, judge is not amused](https://simonwillison.net/2023/May/27/lawyer-chatgpt/) ğŸ”´ | A lawyer cited fake cases generated by ChatGPT in their legal filings, highlighting the importance of verifying AI-generated content for accuracy and legitimacy. While generative AI tech like ChatGPT can benefit the legal industry, it is not perfect and may cause ethical and legal issues if not carefully reviewed. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md) | 2023-06-06 |
| [â€˜Godfather of AIâ€™ says AI threat is â€˜more urgentâ€™ to humanity than climate change](https://nypost.com/2023/05/08/godfather-of-ai-says-its-threat-is-more-urgent-than-climate-change/) ğŸ”´ | AI expert Geoffrey Hinton, â€œGodfather of AI,â€ considers risks of unbridled AI more pressing for humanity than climate change. Hinton warns of threats like job loss and misinformation, but does not support a stoppage of AI development. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2023-05-16 |
| [â€œGodfather of AIâ€ quits Google with regrets and fears about his lifeâ€™s work](https://www.theverge.com/2023/5/1/23706311/hinton-godfather-of-ai-threats-fears-warnings) ğŸ”´ | Geoffrey Hinton who won the â€˜Nobel Prize of computingâ€™ for his trailblazing work on neural networks is now free to speak about the risks of AI. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [Google ğŸ”](Topic_Google.md) | 2023-05-09 |
| [Samsung workers made a major error by using ChatGPT](https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt) ğŸ”´ | Samsung workers leaked proprietary data through ChatGPT. OpenAI may now possess trade secrets. The incident raised alarms about data privacy and GDPR rule. | [AI safety ğŸ”](Topic_AI_safety.md), [AI and copyright Â©ï¸](Topic_AI_and_copyright.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-04-11 |
| [Defamed by ChatGPT: My Own Bizarre Experience with Artificiality of â€œArtificial Intelligenceâ€](https://jonathanturley.org/2023/04/06/defamed-by-chatgpt-my-own-bizarre-experience-with-artificiality-of-artificial-intelligence/) ğŸ”´ | AI program ChatGPT created false claim of sexual harassment w/o factual basis, warns of biases & dangers of AI in tackling disinformation. | [AI safety ğŸ”](Topic_AI_safety.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-04-11 |
| [They thought loved ones were calling for help. It was an AI scam.](https://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/) ğŸ”´ |  | [AI safety ğŸ”](Topic_AI_safety.md), [Text-to-speech ğŸ“¢](Topic_Text-to-speech.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2023-03-13 |
| [Planning for AGI and beyond](https://openai.com/blog/planning-for-agi-and-beyond) ğŸŸ¢ | OpenAIâ€™s steps in making sure that AGI will benefit anyone. | [AI safety ğŸ”](Topic_AI_safety.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-03-06 |
| [How should AI systems behave, and who should decide](https://openai.com/blog/how-should-ai-systems-behave/) ğŸŸ¢ | a summary of how ChatGPTâ€™s behavior is shaped and how OpenAI plans to improve ChatGPTâ€™s default behavior. | [AI safety ğŸ”](Topic_AI_safety.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-02-20 |