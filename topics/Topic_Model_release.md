# Model release news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [OpenAI releases o1-preview model](https://openai.com/index/introducing-openai-o1-preview/) ğŸŸ¢ | OpenAI introduces the o1-preview, the first in a new series of reasoning models significantly adept at complex tasks in science, coding, and math. These models outperform predecessors by employing advanced reasoning before responding, with test performances comparable to PhD students in rigorous fields. Despite lacking some GPT-4o features, o1-preview excels in specialized reasoning tasks, promising substantial AI advancements. | [Model release ğŸ‰](Topic_Model_release.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-09-16 |
| [Mistral releases Pixtral 12B, its first multimodal model](https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/) ğŸŸ¢ | Mistral has introduced Pixtral 12B, a 12-billion-parameter multimodal AI model that processes both text and images. Building on features from their previous text model, Nemo 12B, Pixtral 12B is available for free download on GitHub and Hugging Face under an Apache 2.0 license. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-09-16 |
| [Alibaba releases new AI model Qwen2-VL that can analyze videos more than 20 minutes long](https://venturebeat.com/ai/alibaba-releases-new-ai-model-qwen2-vl-that-can-analyze-videos-more-than-20-minutes-long/) ğŸŸ¢ | Alibaba Cloudâ€™s new AI model, Qwen2-VL, excels in video analysis and multilingual comprehension, outperforming Metaâ€™s Llama 3.1 and Googleâ€™s Gemini-1.5 in benchmarks. It supports multiple languages and extended video content analysis, and is available in three sizes, with two being open-sourced. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md) | 2024-09-09 |
| [OpenAIâ€™s Strawberry AI is reportedly the secret sauce behind next-gen Orion language model](https://the-decoder.com/openais-strawberry-ai-is-reportedly-the-secret-sauce-behind-next-gen-orion-language-model/) ğŸŸ¢ | OpenAI is working on â€œStrawberry,â€ an AI model focused on solving math and programming challenges, aimed at supporting â€œOrion,â€ the anticipated successor to GPT-4. Strawberry is slated for release in the fall and may augment ChatGPT with improved data generation and search abilities, having already displayed promising performance in tests and to national security stakeholders. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-09-02 |
| [Runwayâ€™s Gen-3 Alpha Turbo is here and can make AI videos faster than you can type](https://venturebeat.com/ai/runways-gen-3-alpha-turbo-is-here-and-can-make-ai-videos-faster-than-you-can-type/) ğŸŸ¢ | Runway ML introduces Gen-3 Alpha Turbo, an AI video generation model delivering 7x speed improvements and 50% cost reduction. Widely available across subscription plans, the model addresses diverse needs while promising advancements amidst ethical scrutiny, signaling Runwayâ€™s ambition for market leadership. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-08-26 |
| [Introducing SAM 2: The next generation of Meta Segment Anything Model for videos and images](https://ai.meta.com/blog/segment-anything-2/) ğŸŸ¢ | Meta has launched SAM 2, an improved AI model for prompt-based real-time video and image segmentation, featuring zero-shot learning and requiring three times fewer interactions. SAM 2 is now available as open-source under the Apache 2.0 license. | [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Meta â™¾](Topic_Meta.md) | 2024-08-06 |
| [Google releases Gemma 2 2B](https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/) ğŸŸ¢ | Gemma 2, the latest AI release, features three key models: the efficient Gemma 2 2B which outperforms GPT-3.5 models in the Chatbot Arena, ShieldGemma for enhanced safety content classification, and Gemma Scope which provides advanced model interpretability. | [Model release ğŸ‰](Topic_Model_release.md), [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md), [Google ğŸ”](Topic_Google.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2024-08-06 |
| [Meta releases Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) ğŸŸ¢ | Meta unveiled the Llama 3.1 405B model, a cutting-edge open-source large language AI with advanced multilingual, reasoning, and tool-use features, alongside improvements to its 8B and 70B models. The update offers extended context lengths, better training, and thorough evaluations, available for download on various platforms. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2024-07-29 |
| [Mistral releases Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) ğŸŸ¢ | Mistral launched their new model, Mistral Large 2, with 123 billion parameters and a 128k context window, offering multi-language and programming language support, optimized for high-throughput single-node inference. It delivers 84.0% accuracy on the MMLU benchmark, exhibits enhanced code generation, and reasoning capabilities. The model is available with research and commercial licensing options. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-07-29 |
| [GPT-4o mini: advancing cost-efficient intelligence](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) ğŸŸ¢ | OpenAI has released GPT-4o mini, an advanced, cost-efficient AI model priced at $0.15/million input tokens and $0.60/million output tokens, offering superior performance at a lower cost than GPT-3.5 Turbo. | [Model release ğŸ‰](Topic_Model_release.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-07-22 |
| [Mistral NeMo](https://mistral.ai/news/mistral-nemo/) ğŸŸ¢ | Mistral, in collaboration with NVIDIA, has launched the 12B parameter Mistral NeMo model, featuring a 128k token context window, FP8 inference compatibility, and a cutting-edge Tekken tokenizer. It is open-sourced under Apache 2.0, boasts enhanced multilingual capabilities, and outperforms the previous 7B version in instruction-following tasks. | [Model release ğŸ‰](Topic_Model_release.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-07-22 |
| [Codestral Mamba](https://mistral.ai/news/codestral-mamba/) ğŸŸ¢ | Mistral has introduced Codestral Mamba, a new coding-centric Mamba model known for efficiently managing long sequences with linear time inference and theoretical support for unlimited sequence lengths. It competes with leading SOTA models and is open-source, accessible for extension through the GitHub repository with integration options like mistral-inference SDK, TensorRT-LLM, and an upcoming llama.cpp. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-07-22 |
| [Meta to drop Llama 3 400b next week â€” hereâ€™s why you should care](https://www.tomsguide.com/ai/meta-to-drop-llama-3-400b-next-week-heres-why-you-should-care) ğŸŸ¢ | Meta plans to launch Llama 3 400B in July 2024, expanding the Llama 3 AI model series. This open-source model will offer improved features for chatbots and multilingual applications, aiming to provide wide access to the latest AI advancements. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2024-07-22 |
| [Stability AI releases a sound generator](https://techcrunch.com/2024/06/05/stability-ai-releases-a-sound-generator/) ğŸŸ¢ | Stability AI has launched â€œStable Audio Open,â€ an AI model that generates sound from text descriptions using royalty-free samples, geared towards non-commercial use. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Stability AI âš–ï¸](Topic_Stability_AI.md) | 2024-06-10 |
| [Mistral releases Codestral](https://mistral.ai/news/codestral/) ğŸŸ¢ | Codestral is Mistral AIâ€™s new generative AI model focused on coding, boasting proficiency in over 80 programming languages and a large 32k context window for superior performance in benchmarks. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-06-03 |
| [Microsoft introduces Phi-Silica, a 3.3B parameter model made for Copilot+ PC NPUs](https://venturebeat.com/ai/microsoft-introduces-phi-silica-a-3-3b-parameter-model-made-for-copilot-pc-npus/) ğŸŸ¢ | Microsoft has unveiled Phi-Silica, a compact language model with 3.3 billion parameters, tailored for Copilot+ PCs equipped with NPUs. This model is engineered for rapid on-device inferencing, improving productivity and accessibility for Windows users with optimal power efficiency. Phi-Silica is Microsoftâ€™s inaugural local language model, with a release slated for June. | [Model release ğŸ‰](Topic_Model_release.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md) | 2024-05-27 |
| [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) ğŸŸ¢ | Mistral has launched version 3 of their 7B model, the models â€œMistral-7B-v0.3â€ and â€œMistral-7B-Instruct-v0.3â€. Enhancements include an expanded vocabulary of 32,768 terms, integration with the v3 Tokenizer, and new function calling capabilities. | [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-05-27 |
| [OpenAI releases GPT-4o](https://openai.com/index/spring-update/) ğŸŸ¢ | OpenAI released the new model GPT-4o, capable of processing and generating text, audio, and image inputs and outputs. It boasts quick audio response times on par with humans, enhanced non-English language processing, and cost-efficient API usage, while maintaining GPT-4 Turboâ€™s performance levels. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-05-21 |
| [IBMâ€™s Granite code model family is going open source](https://research.ibm.com/blog/granite-code-models-open-source) ğŸŸ¢ | IBM has released its Granite code models as open source. These models, trained on 116 languages with up to 34 billion parameters, facilitate code generation, bug fixing, and explanation tasks, and are accessible via GitHub and Hugging Face under the Apache 2.0 license. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-05-21 |
| [DeepMind releases AlphaFold 3](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/#life-molecules) ğŸŸ¢ | AlphaFold 3 is an advanced AI model by Google DeepMind and Isomorphic Labs, capable of accurately predicting biomolecular structures and interactions. Providing a significant advancement over prior models, it enhances scientific research and drug development, and is available globally through the AlphaFold Server. | [AI in healthcare ğŸ¥](Topic_AI_in_healthcare.md), [Model release ğŸ‰](Topic_Model_release.md), [DeepMind ğŸ§©](Topic_DeepMind.md), [Google ğŸ”](Topic_Google.md) | 2024-05-13 |
| [Apple releases OpenELM: small, open source AI models designed to run on-device](https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/) ğŸŸ¢ | Apple has introduced OpenELM, a suite of open-source AI text generation models with 270M to 3B parameters, optimized for on-device deployment. Available on Hugging Face, these models are released under a sample code license to enable AI functionalities independently of the cloud. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Apple ğŸ](Topic_Apple.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-04-29 |
| [Snowflake releases Arctic, an open LLM for Enterprise AI](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/) ğŸŸ¢ | Snowflake AI Research has released Arctic, a cost-effective enterprise AI LLM featuring a Dense-MoE Hybrid transformer architecture with 480 billion parameters. Trained for less than $2 million, Arctic excels in tasks like SQL generation and coding. Itâ€™s fully open-source under Apache 2.0, providing free access to model weights and code. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Funding ğŸ’°](Topic_Funding.md) | 2024-04-29 |
| [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) ğŸŸ¢ | Meta has introduced Meta Llama 3, a state-of-the-art open-source large language model (LLM) with versions up to 70 billion parameters, providing enhanced reasoning and multilingual capabilities. The current best models are pretrained and instruction-fine-tuned at both 8B and 70B scales. Additionally, even larger models exceeding 400 billion parameters are in development, promising to push the boundaries further upon their release in the coming months. | [Model release ğŸ‰](Topic_Model_release.md), [Meta â™¾](Topic_Meta.md) | 2024-04-23 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) ğŸŸ¢ | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-04-23 |
| [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) ğŸŸ¢ | Databricks has unveiled DBRX, a new open-source large language model (LLM) that surpasses GPT-3.5 in programming and general tasks, and is competitive with Gemini 1.0 Pro. DBRX features a mixture-of-experts architecture with a massive 132 billion parameters, though only 36 billion are active for any given input. Both DBRX Base and DBRX Instruct models are openly accessible on Hugging Face. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2024-04-02 |
| [Grok open release](https://github.com/xai-org/grok-1) ğŸŸ¢ | xAI has released Grok-1, a language Mixture-of-Experts model with 314 billion parameters, following its pre-training in October 2023. This base model checkpoint is intended for further research and the development of conversational applications, and it is accessible under the Apache 2.0 license. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [Grok ğŸ¦](Topic_Grok.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-03-18 |
| [Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) ğŸŸ¢ | Anthropic has launched Claude 3, a new AI that surpasses GPT-4, with three models: Opus, Sonnet, and Haiku. Each supports a 200k context window, vision abilities, and multiple languages. Opus is touted as the top performer. Sonnet is integrated with Amazon Bedrock and Google Cloudâ€™s Vertex AI, while Opus and Haiku are slated for future release along with new features like function calling and REPL. | [Claude ğŸ–‹ï¸](Topic_Claude.md), [Anthropic ğŸŒ](Topic_Anthropic.md), [Model release ğŸ‰](Topic_Model_release.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Google ğŸ”](Topic_Google.md), [Amazon ğŸŒ³](Topic_Amazon.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-11 |
| [Inflection-2.5: meet the worldâ€™s best personal AI](https://inflection.ai/inflection-2-5) ğŸŸ¢ | Inflection has launched its latest AI version, Inflection-2.5, enhancing its AI model, Pi, with advanced cognitive capabilities that challenge leading language models like GPT-4. Notably, Inflection-2.5 achieves competitive performance in AI tasks, particularly in coding and math, with 40% less computational power required during its training phase. In addition to its improved processing efficiency, Pi now features the ability to conduct real-time web searches to provide updated news and information. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-11 |
| [Mistral AI releases new model to rival GPT-4 and its own chat assistant](https://techcrunch.com/2024/02/26/mistral-ai-releases-new-model-to-rival-gpt-4-and-its-own-chat-assistant/) ğŸŸ¢ | Mistral has launched Mistral Large, ranking just below GPT4. It boasts a 32K token context window and multilingual support for English, French, Spanish, German, and Italian. The model excels in following precise instructions, allowing for tailored moderation policies. | [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md) | 2024-03-04 |
| [Gemma â€” a family of lightweight, state-of-the art open models from Google.](https://ai.google.dev/gemma/) ğŸŸ¢ | Google has released Gemma, an open-source large language model based on Gemini, in two versions with 2 billion (2B) and 7 billion (7B) parameters. Both versions come with a basic pretrained model and an instruction-tuned variant to enhance performance. | [Model release ğŸ‰](Topic_Model_release.md), [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md), [Google ğŸ”](Topic_Google.md) | 2024-02-26 |
| [OpenAI releases new embedding models and API updates](https://openai.com/blog/new-embedding-models-and-api-updates) ğŸŸ¢ | OpenAI has announced updates to their AI model suite, including the launch of more efficient embedding models and cost-reduced versions of GPT-3.5 Turbo and a new GPT-4 Turbo model. The â€œtext-embedding-3-largeâ€ leads with a 64.6% MTEB score at $0.00013 per 1k tokens, while the â€œtext-embedding-3-smallâ€ offers improved performance over its predecessor at a fivefold cost reduction. Additionally, the â€œgpt-3.5-turbo-0125â€ is now 50% cheaper, priced at $0.0005 per 1k tokens, and a new â€œgpt-4â€“0125-previewâ€ model has been introduced. | [Model release ğŸ‰](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-01-29 |
| [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/) ğŸŸ¢ | LangChain has released its first stable and backward-compatible version, v0.1.0. This release brings better observability and debugging capabilities, including performance tracking and insight tools, and introduces a new versioning system for clear API and feature updates. | [LangChain and LlamaIndex ğŸ”—](Topic_LangChain_and_LlamaIndex.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-01-15 |
| [Midjourney v6 release](https://mid-journey.ai/midjourney-v6-release/) ğŸŸ¢ | The latest update for Midjourney, version 6, introduces features such as enhanced prompt accuracy and length, increased coherence, better image prompting, and an improved remix mode. Additionally, a new feature for minor text drawing has been added, which can be utilized by including text within quotations. | [Midjourney ğŸ›¤ï¸](Topic_Midjourney.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-01-02 |
| [Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) ğŸŸ¢ | Microsoft has released Phi-2, a language model with 2.7 billion parameters that outperforms models up to 25 times its size. Phi-2 achieves remarkable reasoning and language understanding abilities using high-quality data and synthetic datasets. It outperforms larger models on challenging benchmarks, especially in tasks like coding and math. | [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Microsoft ğŸªŸ](Topic_Microsoft.md) | 2023-12-19 |
| [Introducing gigaGPT: GPT-3 sized models in 565 lines of code](https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code) ğŸŸ¢ | Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models. | [Model release ğŸ‰](Topic_Model_release.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-19 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) ğŸŸ¢ | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistralâ€™s beta API and dev platform. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-11 |
| [Kyutai is a French AI research lab with a $330 million budget that will make everything open source](https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/) ğŸŸ¢ | Paris-based AI research lab, Kyutai, secures $330 million in funding to advance the development of artificial general intelligence. With these resources, Kyutai plans to conduct comprehensive research led by PhD students, postdocs, and researchers. Additionally, the lab prioritizes transparency in AI by openly sharing their models, source code, and data. | [Funding ğŸ’°](Topic_Funding.md), [Model release ğŸ‰](Topic_Model_release.md), [AI datasets ğŸ“Š](Topic_AI_datasets.md) | 2023-11-20 |
| [Jina AI Launches Worldâ€™s First Open-Source 8K Text Embedding, Rivaling OpenAI](https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/) ğŸŸ¢ | Jina AI introduces jina-embeddings-v2, an open-source embedding model that supports 8K context length. It matches OpenAIâ€™s 8K model in key areas like Classification Average, Reranking Average, Retrieval Average, and Summarization Average in the MTEB leaderboard. | [Model release ğŸ‰](Topic_Model_release.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-10-30 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) ğŸŸ¢ | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its modelâ€™s knowledge base. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-10-10 |
| [Adobeâ€™s Firefly generative AI models are now generally available](https://techcrunch.com/2023/09/13/adobes-firefly-generative-ai-models-are-now-generally-available-get-pricing-plans/) ğŸŸ¢ | Adobe has released commercially available generative AI models in their Creative Cloud, including a standalone web app called Firefly. The new â€œgenerative creditsâ€ system controls user interactions with Fireflyâ€™s AI models, with each click on â€˜generateâ€™ using one credit. | [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md) | 2023-09-18 |
| [Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b) ğŸŸ¢ | TII has just released Falcon 180B, a powerful language model with 180 billion parameters trained on 3.5 trillion tokens. Outperforming Llama 2 70B and GPT-3.5 on MMLU, Falcon 180B performs great and ranks high on the Hugging Face Leaderboard. This model is available for commercial use but has strict terms excluding â€œhosting use.â€ | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Model release ğŸ‰](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-09-11 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) ğŸŸ¢ | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-09-11 |
| [AudioCraft: A simple one-stop shop for audio modeling](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) ğŸŸ¢ | Meta has released the code and weights for their AudioCraft models, including MusicGen and AudioGen. These models generate music and audio respectively, based on text-based user inputs. The release also includes the EnCodec decoder, which improves music quality. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Meta â™¾](Topic_Meta.md) | 2023-08-07 |
| [NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data](https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model) ğŸŸ¢ | NASA and IBM Research have collaborated to release the HLS Geospatial FM, an open-source geospatial AI model for Earth observation data. This model has shown success in various applications such as flood mapping, burn scar identification, and predicting crop yields. | [AI datasets ğŸ“Š](Topic_AI_datasets.md), [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md) | 2023-08-07 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) ğŸŸ¢ | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release ğŸ‰](Topic_Model_release.md), [Stability AI âš–ï¸](Topic_Stability_AI.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) ğŸŸ¢ | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md) | 2023-07-24 |
| [GPT-4 API general availability and deprecation of older models in the Completions API](https://openai.com/blog/gpt-4-api-general-availability) ğŸŸ¢ | OpenAI has announced the general availability of the GPT-4 API and the automatic upgrade of GPT-3 models to new models from January 4, 2024. Developers using text-davinci-003 are advised to upgrade to gpt-3.5-turbo-instruct and specify it as the â€œmodelâ€ in API requests for a smooth transition. OpenAI also offers priority access to GPT-3.5 Turbo and GPT-4 fine-tuning for users with fine-tuned older models, understanding the challenges of transitioning from these models. | [Model release ğŸ‰](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-07-10 |
| [Function calling and other API updates](https://openai.com/blog/function-calling-and-other-api-updates) ğŸŸ¢ | OpenAI has released several new updates, including a 16k context version of GPT-3.5-turbo, a new API for calling user-defined functions, cost reductions for models and input tokens, and updated versions of GPT-4 and GPT-3.5-turbo. The updates aim to improve GPTâ€™s capabilities and make it easier to connect with external tools and APIs. | [Model release ğŸ‰](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-06-20 |
| [RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks](https://www.together.xyz/blog/redpajama-7b) ğŸŸ¢ | Introducing the new RedPajama-INCITE models, optimized for few-shot tasks, which outperform similar models on HELM benchmarks. The project analyzed differences with previous models and incorporated community feedback. The models are available under Apache 2.0 license for AI professionals. | [Model release ğŸ‰](Topic_Model_release.md) | 2023-06-12 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) ğŸŸ¢ | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) ğŸŸ¢ | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-05-09 |
| [Hugging Face and ServiceNow release a free code-generating model](https://techcrunch.com/2023/05/04/hugging-face-and-servicenow-release-a-free-code-generating-model/) ğŸŸ¢ | AI startup Hugging Face and ServiceNow Research have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHubâ€™s Copilot. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md) | 2023-05-09 |
| [Meta introduced Segment Anything: Working toward the first foundation model for image segmentation](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/) ğŸŸ¢ | Introducing Segment Anything: democratizing image segmentation with SAM â€” a versatile, promptable model trained on a versatile dataset under Apache 2.0. | [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md), [Meta â™¾](Topic_Meta.md) | 2023-04-11 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) ğŸŸ¢ | Article explains release of Alpaca-30B, â€œinstruction-tunedâ€ version of Facebookâ€™s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2023-03-27 |
| [A New Open Source Flan 20B with UL2](https://www.yitay.net/blog/flan-ul2-20b) ğŸŸ¢ | A UL2 model finetuned on the FLAN dataset (instruction tuning). An alternative to Flan-T5. | [AI datasets ğŸ“Š](Topic_AI_datasets.md), [Model release ğŸ‰](Topic_Model_release.md) | 2023-03-06 |
| [Hugging Face releases SpeechT5](https://huggingface.co/blog/speecht5) ğŸŸ¢ | a model able to do speech-to-text, text-to-speech, and speech-to-speech. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Text-to-speech ğŸ“¢](Topic_Text-to-speech.md), [Speech-to-text ğŸ¤](Topic_Speech-to-text.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [Model release ğŸ‰](Topic_Model_release.md) | 2023-02-13 |