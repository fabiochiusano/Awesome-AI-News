# LLaMA news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Meta releases Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) ğŸŸ¢ | Meta unveiled the Llama 3.1 405B model, a cutting-edge open-source large language AI with advanced multilingual, reasoning, and tool-use features, alongside improvements to its 8B and 70B models. The update offers extended context lengths, better training, and thorough evaluations, available for download on various platforms. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2024-07-29 |
| [Meta to drop Llama 3 400b next week â€” hereâ€™s why you should care](https://www.tomsguide.com/ai/meta-to-drop-llama-3-400b-next-week-heres-why-you-should-care) ğŸŸ¢ | Meta plans to launch Llama 3 400B in July 2024, expanding the Llama 3 AI model series. This open-source model will offer improved features for chatbots and multilingual applications, aiming to provide wide access to the latest AI advancements. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2024-07-22 |
| [Metaâ€™s open source GPT-4 competitor Llama 3 is coming soon](https://the-decoder.com/metas-open-source-gpt-4-competitor-llama-3-is-coming-soon/) ğŸŸ¢ | Meta is set to release Llama 3, an AI assistant intended to outperform its predecessors and compete with OpenAIâ€™s GPT-4. It will debut with two preliminary versions before launching a comprehensive multimodal iteration in the summer. | [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-04-15 |
| [Hugging Face launches open source AI assistant maker to rival OpenAIâ€™s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) ğŸŸ¢ | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAIâ€™s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistralâ€™s Mixtral and Metaâ€™s Llama 2. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-02-05 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) ğŸŸ¢ | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its modelâ€™s knowledge base. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-10-10 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) ğŸŸ¢ | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-09-11 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) ğŸŸ¢ | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release ğŸ‰](Topic_Model_release.md), [Stability AI âš–ï¸](Topic_Stability_AI.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) ğŸŸ¢ | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md) | 2023-07-24 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) ğŸŸ¢ | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) ğŸŸ¢ | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-05-09 |
| [RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens](https://www.together.xyz/blog/redpajama) ğŸŸ¢ | RedPajama aims to reproduce LLaMA models with a 7B parameter model, using a filtered dataset of 1.2T tokens. Their goal is open-source reproducibility. | [AI datasets ğŸ“Š](Topic_AI_datasets.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-04-24 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) ğŸŸ¢ | Article explains release of Alpaca-30B, â€œinstruction-tunedâ€ version of Facebookâ€™s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2023-03-27 |