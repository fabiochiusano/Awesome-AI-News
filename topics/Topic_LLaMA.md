# LLaMA news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Meta is using more than 100,000 Nvidia H100 AI GPUs to train Llama-4](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen) ğŸŸ¢ | Meta is utilizing over 100,000 Nvidia H100 AI GPUs to develop Llama 4, an advanced AI model with improved modalities and reasoning capabilities, positioning itself competitively against Microsoft and Google. Despite the significant power demands, Meta plans to release Llama models for free to encourage broader development and application. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-11-11 |
| [Nvidia just dropped a new AI model on the level of OpenAIâ€™s GPT-4](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/) ğŸŸ¢ | Nvidia has introduced the Llama-3.1-Nemotron-70B-Instruct AI model, outperforming GPT-4 in benchmarks. Built on Metaâ€™s Llama 3.1, it features advanced training for enhanced language capabilities. This marks Nvidiaâ€™s strategic pivot from GPU manufacturing to AI software, potentially reshaping the industry with powerful, customizable AI solutions and challenging current AI leaders. | [Model release ğŸ‰](Topic_Model_release.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-10-21 |
| [Zyphra releases Zamba2â€“7B](https://zyphra.webflow.io/post/zamba2-7b) ğŸŸ¢ | Zyphra has introduced Zamba2â€“7B, a 7B-scale language model that outperforms competitors such as Mistral, Googleâ€™s Gemma, and Metaâ€™s Llama3. It features innovative Mamba2 blocks and dual shared attention layers, resulting in improved inference speed and reduced memory usage. | [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md), [Google ğŸ”](Topic_Google.md) | 2024-10-21 |
| [Meta releases Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) ğŸŸ¢ | Llama 3.2 features advanced AI models, including vision LLMs (11B and 90B) and lightweight text-only models (1B and 3B), optimized for edge and mobile devices. These models excel in tasks such as summarization and image understanding, supporting extensive token lengths. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-09-30 |
| [Introducing Cerebras Inference: AI at Instant Speed](https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed) ğŸŸ¢ | Cerebras has achieved a significant speed advantage in AI language model inference, delivering 1,800 tokens per second on Llama3.1 8B and 450 tokens per second on Llama3.1 70B models, outperforms NVIDIAâ€™s GPU-based solutions by 20-fold and is 2.4 times faster than Groq for the 8B model. Notably, Cerebras stands alone in offering immediate responses at a rate of 450 tokens per second on the 70B model. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2024-09-02 |
| [Zuckerberg says Meta will need 10x more computing power to train Llama 4 than Llama 3](https://techcrunch.com/2024/08/01/zuckerberg-says-meta-will-need-10x-more-computing-power-to-train-llama-4-than-llama-3/) ğŸ”´ | Metaâ€™s CEO, Mark Zuckerberg, has stated that their upcoming language model, Llama 4, will require a tenfold increase in computing power for training compared to its predecessor, Llama 3, suggesting significant capital expenditure on infrastructure. However, CFO Susan Li clarified that these AI advancements are not anticipated to yield substantial revenue in the near term. | [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2024-08-12 |
| [Meta releases Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) ğŸŸ¢ | Meta unveiled the Llama 3.1 405B model, a cutting-edge open-source large language AI with advanced multilingual, reasoning, and tool-use features, alongside improvements to its 8B and 70B models. The update offers extended context lengths, better training, and thorough evaluations, available for download on various platforms. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2024-07-29 |
| [Meta to drop Llama 3 400b next week â€” hereâ€™s why you should care](https://www.tomsguide.com/ai/meta-to-drop-llama-3-400b-next-week-heres-why-you-should-care) ğŸŸ¢ | Meta plans to launch Llama 3 400B in July 2024, expanding the Llama 3 AI model series. This open-source model will offer improved features for chatbots and multilingual applications, aiming to provide wide access to the latest AI advancements. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2024-07-22 |
| [Metaâ€™s open source GPT-4 competitor Llama 3 is coming soon](https://the-decoder.com/metas-open-source-gpt-4-competitor-llama-3-is-coming-soon/) ğŸŸ¢ | Meta is set to release Llama 3, an AI assistant intended to outperform its predecessors and compete with OpenAIâ€™s GPT-4. It will debut with two preliminary versions before launching a comprehensive multimodal iteration in the summer. | [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md) | 2024-04-15 |
| [Hugging Face launches open source AI assistant maker to rival OpenAIâ€™s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) ğŸŸ¢ | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAIâ€™s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistralâ€™s Mixtral and Metaâ€™s Llama 2. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-02-05 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) ğŸŸ¢ | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its modelâ€™s knowledge base. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-10-10 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) ğŸŸ¢ | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-09-11 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) ğŸŸ¢ | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release ğŸ‰](Topic_Model_release.md), [Stability AI âš–ï¸](Topic_Stability_AI.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) ğŸŸ¢ | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md) | 2023-07-24 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) ğŸŸ¢ | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) ğŸŸ¢ | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-05-09 |
| [RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens](https://www.together.xyz/blog/redpajama) ğŸŸ¢ | RedPajama aims to reproduce LLaMA models with a 7B parameter model, using a filtered dataset of 1.2T tokens. Their goal is open-source reproducibility. | [AI datasets ğŸ“Š](Topic_AI_datasets.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2023-04-24 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) ğŸŸ¢ | Article explains release of Alpaca-30B, â€œinstruction-tunedâ€ version of Facebookâ€™s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release ğŸ‰](Topic_Model_release.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md) | 2023-03-27 |