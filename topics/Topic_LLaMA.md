# LLaMA news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Metaâ€™s open source GPT-4 competitor Llama 3 is coming soon](https://the-decoder.com/metas-open-source-gpt-4-competitor-llama-3-is-coming-soon/) ğŸŸ¢ | Meta is set to release Llama 3, an AI assistant intended to outperform its predecessors and compete with OpenAIâ€™s GPT-4. It will debut with two preliminary versions before launching a comprehensive multimodal iteration in the summer. | [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [Meta â™¾](topics/Topic_Meta_â™¾.md), [GPT-4 and GPT-4 turbo ğŸš€](topics/Topic_GPT-4_and_GPT-4_turbo_ğŸš€.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md), [Multimodal AI (image, video, audio) ğŸ“¸](topics/Topic_Multimodal_AI_(image_video_audio)_ğŸ“¸.md) | 2024-04-15 |
| [Hugging Face launches open source AI assistant maker to rival OpenAIâ€™s custom GPTs](https://venturebeat.com/ai/hugging-face-launches-open-source-ai-assistant-maker-to-rival-openais-custom-gpts/) ğŸŸ¢ | Hugging Face has introduced free, customizable Chat Assistants on its Hugging Chat platform, presenting an open-source alternative to OpenAIâ€™s GPT services. This initiative offers developers and AI enthusiasts cost-free access to various large language models, including Mistralâ€™s Mixtral and Metaâ€™s Llama 2. | [Hugging Face ğŸ¤—](topics/Topic_Hugging_Face_ğŸ¤—.md), [Mistral ğŸŒ¬ï¸](topics/Topic_Mistral_ğŸŒ¬ï¸.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [Meta â™¾](topics/Topic_Meta_â™¾.md), [ChatGPT ğŸ’¬](topics/Topic_ChatGPT_ğŸ’¬.md), [OpenAI ğŸŒŸ](topics/Topic_OpenAI_ğŸŒŸ.md) | 2024-02-05 |
| [Meta quietly unveils Llama 2 Long AI that beats GPT-3.5 Turbo and Claude 2 on some tasks](https://venturebeat.com/ai/meta-quietly-releases-llama-2-long-ai-that-outperforms-gpt-3-5-and-claude-2-on-some-tasks/) ğŸŸ¢ | Meta is releasing Llama 2 Long, an enhanced version of Llama 2 that underwent continual pretraining with longer training sequences and upsampled long texts. By adding 400 billion tokens and making minor changes to the Rotary Positional Embedding (RoPE), Llama 2 Long can now attend to longer information sequences and include less related information in its modelâ€™s knowledge base. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [Meta â™¾](topics/Topic_Meta_â™¾.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-10-10 |
| [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) ğŸŸ¢ | Adept.ai introduces Persimmon-8B, an open-source LLM with impressive performance and a compact size. Trained on less data, it achieves comparable results to LLaMA2 and offers fast C++ implementation combined with flexible Python inference. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md) | 2023-09-11 |
| [Meet FreeWilly, Our Large And Mighty Instruction Fine-Tuned Models](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) ğŸŸ¢ | Stability AI and CarperAI Lab have collaborated to release FreeWilly, a LLaMA 2 model fine-tuned using Supervised Fine-Tune (SFT) techniques. FreeWilly2 performs comparably to GPT-3.5 in certain tasks, and its capabilities have been verified by both Stability AI researchers and Hugging Face. Both models are publicly available under a non-commercial license. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [Stability AI âš–ï¸](topics/Topic_Stability_AI_âš–ï¸.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](topics/Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo_ğŸ’¡.md) | 2023-07-31 |
| [Meta releases Llama 2](https://ai.meta.com/resources/models-and-libraries/llama/) ğŸŸ¢ | Meta has released Llama-2, an open-source model with a commercial license, that showcases similar performance to ChatGPT. Trained on 2T tokens with varying parameter sizes, Llama-2 was further fine-tuned and improved using a combination of instruction and reinforcement learning, outperforming other open-source models like Falcon and MPT. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [Meta â™¾](topics/Topic_Meta_â™¾.md), [ChatGPT ğŸ’¬](topics/Topic_ChatGPT_ğŸ’¬.md) | 2023-07-24 |
| [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, by MosaicML](https://www.mosaicml.com/blog/mpt-7b) ğŸŸ¢ | MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open-source, available for commercial use, and matches the quality of LLaMA-7B. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md) | 2023-05-09 |
| [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) ğŸŸ¢ | The release includes the first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md) | 2023-05-09 |
| [RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens](https://www.together.xyz/blog/redpajama) ğŸŸ¢ | RedPajama aims to reproduce LLaMA models with a 7B parameter model, using a filtered dataset of 1.2T tokens. Their goal is open-source reproducibility. | [AI datasets ğŸ“Š](topics/Topic_AI_datasets_ğŸ“Š.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md) | 2023-04-24 |
| [Releasing Alpaca-30B](https://abuqader.substack.com/p/releasing-alpaca-30b) ğŸŸ¢ | Article explains release of Alpaca-30B, â€œinstruction-tunedâ€ version of Facebookâ€™s Llama model, benefits of fine-tuning, usage steps and community involvement. | [Model release ğŸ‰](topics/Topic_Model_release_ğŸ‰.md), [LLaMA ğŸ¦™](topics/Topic_LLaMA_ğŸ¦™.md), [Meta â™¾](topics/Topic_Meta_â™¾.md) | 2023-03-27 |