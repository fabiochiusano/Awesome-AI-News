# AI Chips and GPUs news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Anthropic raises another $4B from Amazon, makes AWS its â€œprimaryâ€ training partner](https://techcrunch.com/2024/11/22/anthropic-raises-an-additional-4b-from-amazon-makes-aws-its-primary-cloud-partner/) ğŸŸ¢ | Anthropic raised $4 billion from Amazon, making AWS its main AI training partner. They collaborate on Trainium accelerator development and integrate Claude models on Amazonâ€™s platform. Amazonâ€™s investment totals $8 billion, creating regulatory interest. | [Amazon ğŸŒ³](Topic_Amazon.md), [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Claude ğŸ–‹ï¸](Topic_Claude.md), [Anthropic ğŸŒ](Topic_Anthropic.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2024-11-25 |
| [OpenAI and rivals seek new path to smarter AI as current methods hit limitations](https://finance.yahoo.com/news/openai-rivals-seek-path-smarter-100616130.html) ğŸŸ¢ | AI companies like OpenAI are innovating training techniques to overcome limits of large language models. Techniques behind OpenAIâ€™s o1 model focus on human-like reasoning, shifting away from â€œbigger is better.â€ This approach may reshape AI resources and impact Nvidiaâ€™s dominance in AI chips. Rivals are also pursuing similar advancements. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-11-18 |
| [Amazon ready to use its own AI chips, reduce its dependence on Nvidia](https://arstechnica.com/ai/2024/11/amazon-ready-to-use-its-own-ai-chips-reduce-its-dependence-on-nvidia/) ğŸŸ¢ | Amazon is set to launch Trainium 2 AI chips, aiming to reduce reliance on Nvidia and cut costs for Amazon Web Services (AWS) customers. These chips promise efficiency and savings, attracting users like Anthropic and Databricks. This move highlights a growing trend of tech giants developing custom chips to drive AI growth. | [Amazon ğŸŒ³](Topic_Amazon.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Anthropic ğŸŒ](Topic_Anthropic.md) | 2024-11-18 |
| [Meta is using more than 100,000 Nvidia H100 AI GPUs to train Llama-4](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen) ğŸŸ¢ | Meta is utilizing over 100,000 Nvidia H100 AI GPUs to develop Llama 4, an advanced AI model with improved modalities and reasoning capabilities, positioning itself competitively against Microsoft and Google. Despite the significant power demands, Meta plans to release Llama models for free to encourage broader development and application. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [LLaMA ğŸ¦™](Topic_LLaMA.md), [Meta â™¾](Topic_Meta.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-11-11 |
| [Introducing the First AMD 1B Language Models: AMD OLMo](https://www.amd.com/en/developer/resources/technical-articles/introducing-the-first-amd-1b-language-model.html) ğŸŸ¢ | AMD has introduced AMD OLMo, a series of 1 billion parameter language models trained on 1.3 trillion tokens using AMD Instinct MI250 GPUs. These open-sourced models excel in reasoning and instruction-following, outperforming similar-sized models in general reasoning and chat benchmarks. | [Model release ğŸ‰](Topic_Model_release.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-11-11 |
| [OpenAI will start using AMD chips and could make its own AI hardware in 2026](https://www.theverge.com/2024/10/29/24282843/openai-custom-hardware-amd-nvidia-ai-chips) ğŸŸ¢ | OpenAI is enhancing its hardware strategy by integrating AMD chips via Microsoft Azure and aims to develop its own AI-specific hardware by 2026. Collaborating with Broadcom and securing production capacity with TSMC, OpenAI still faces challenges in matching the advanced custom AI chip technology of Google, Microsoft, and Amazon. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-11-11 |
| [AMD is turning its back on flagship gaming GPUs to chase AI first](https://www.theverge.com/2024/9/9/24240173/amd-udna-gpu-ai-gaming-rdna-cdna-jack-huynh) ğŸŸ¢ | AMD is prioritizing AI development over flagship gaming GPUs to achieve a larger market share and attract developer support. According to Jack Huynh, the goal is to reach a 40% market share to compete with Nvidia and optimize AMD platforms for developers before potentially re-focusing on gaming GPUs. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-09-16 |
| [Elon Musk is putting his AI chips to work â€” and heâ€™s catching up with Mark Zuckerberg](https://www.businessinsider.com/elon-musk-xai-chips-mark-zuckerberg-2024-9) ğŸŸ¢ | Elon Muskâ€™s xAI has launched Colossus, a major training cluster boasting 100,000 Nvidia H100 GPUs, making it the worldâ€™s most powerful AI system. Built in 122 days in Memphis and set to double in capacity soon, this development comes amid a global GPU shortage, with rivals like Meta and Microsoft also competing for AI supremacy. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Meta â™¾](Topic_Meta.md), [Microsoft ğŸªŸ](Topic_Microsoft.md) | 2024-09-09 |
| [Introducing Cerebras Inference: AI at Instant Speed](https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed) ğŸŸ¢ | Cerebras has achieved a significant speed advantage in AI language model inference, delivering 1,800 tokens per second on Llama3.1 8B and 450 tokens per second on Llama3.1 70B models, outperforms NVIDIAâ€™s GPU-based solutions by 20-fold and is 2.4 times faster than Groq for the 8B model. Notably, Cerebras stands alone in offering immediate responses at a rate of 450 tokens per second on the 70B model. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [LLaMA ğŸ¦™](Topic_LLaMA.md) | 2024-09-02 |
| [AMD to acquire infrastructure player ZT Systems for $4.9B to amp up its AI ecosystem play](https://techcrunch.com/2024/08/19/amd-to-acquire-infrastructure-player-zt-systems-for-4-9b-to-amp-up-its-ai-ecosystem-play/) ğŸŸ¢ | AMD has acquired AI-focused infrastructure company ZT Systems for $4.9 billion, aiming to enhance their data center offerings with ZTâ€™s specialized systems design expertise in AI applications. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-08-26 |
| [AMD is becoming an AI chip company, just like Nvidia](https://www.theverge.com/2024/7/30/24209938/amd-q2-2024-earnings-datacenter-ai-revenue) ğŸŸ¢ | AMDâ€™s Q2 2024 earnings highlight a strategic shift towards AI, with data center products like the Instinct MI300 accelerator leading sales, which have surged by 115%. The MI300 broke $1 billion in quarterly sales, indicating AMDâ€™s intent to annually release AI chips to rival Nvidiaâ€™s market dominance. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-08-12 |
| [Groq Raises $640M To Meet Soaring Demand for Fast AI Inference](https://wow.groq.com/news_press/groq-raises-640m-to-meet-soaring-demand-for-fast-ai-inference/) ğŸŸ¢ | Groq, an AI hardware company, has raised $640 million in a Series D round led by BlackRock, reaching a $2.8 billion valuation. The investment will expand Groqâ€™s capabilities by more than 100,000 LPUs to support growing demand from enterprises and developers, and will enable the company to hire industry experts to drive further growth. | [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-08-12 |
| [Elon Musk: Grok 2 AI Arrives in August](https://www.pcmag.com/news/elon-musk-grok-2-ai-arrives-in-august) ğŸŸ¢ | Elon Musk has unveiled plans for Grok 2, a new AI model expected in August 2024, promising enhanced efficiency. His company anticipates an upgrade to Grok 3 by the end of the same year, utilizing cutting-edge Nvidia GPU technology. | [Grok ğŸ¦](Topic_Grok.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-07-08 |
| [Nvidia shipped 3.76M data center GPUs in 2023 â€” dominates business with 98% revenue share](https://www.tomshardware.com/tech-industry/nvidia-shipped-376m-data-center-gpus-in-2023-dominates-business-with-98-revenue-share) ğŸŸ¢ | In 2023, Nvidia consolidated its position in the data center GPU market with a 98% share by distributing 3.76 million units and achieved a remarkable 126% revenue increase since 2020, reaching $60.9 billion, even amidst U.S. export restrictions and manufacturing hurdles. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-06-17 |
| [AMD unveils new AI chips to compete with Nvidia](https://www.fastcompany.com/91134766/amd-unveils-new-ai-chips-to-compete-with-nvidia) ğŸŸ¢ | AMD is challenging Nvidiaâ€™s leadership in AI with upcoming releases: the MI325X in 2024, and the MI350/MI400 series in 2025â€“2026, promising notable performance boosts to satisfy increasing AI demands. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-06-10 |
| [China invests $47 billion in largest ever chip fund](https://techxplore.com/news/2024-05-china-invests-billion-largest-chip.html) ğŸŸ¢ | China allocated $47.48 billion to a new chip fund aimed at advancing domestic semiconductor production, a critical step toward self-sufficiency and competitiveness in technology sectors, including AI. | [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-06-03 |
| [Nvidia Stock Surges as Sales Forecast Delivers on AI Hopes](https://finance.yahoo.com/news/nvidia-forecast-shatters-estimates-ai-210754051.html) ğŸŸ¢ | Nvidiaâ€™s stock surged 9.3% after a promising sales forecast, pointing to a robust demand for AI technologies. The $28 billion projected Q2 revenue exceeds expectations, highlighting the companyâ€™s strong position in the AI market, buoyed by their new Blackwell chips and significant data-center revenue. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-05-27 |
| [Microsoft introduces Phi-Silica, a 3.3B parameter model made for Copilot+ PC NPUs](https://venturebeat.com/ai/microsoft-introduces-phi-silica-a-3-3b-parameter-model-made-for-copilot-pc-npus/) ğŸŸ¢ | Microsoft has unveiled Phi-Silica, a compact language model with 3.3 billion parameters, tailored for Copilot+ PCs equipped with NPUs. This model is engineered for rapid on-device inferencing, improving productivity and accessibility for Windows users with optimal power efficiency. Phi-Silica is Microsoftâ€™s inaugural local language model, with a release slated for June. | [Model release ğŸ‰](Topic_Model_release.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md) | 2024-05-27 |
| [100 things Google announced at I/O 2024](https://blog.google/technology/ai/google-io-2024-100-announcements/) ğŸŸ¢ | At Google I/O 2024, notable AI developments were announced such as Gemini 1.5 models, Trillium TPU, and enhanced AI in Google Search. Key introductions include Imagen 3 for image creation, Veo for video generation, and upgraded features in the Gemini app for premium users, alongside new generative media tools. | [Google Gemini ğŸŒŒ](Topic_Google_Gemini.md), [Google ğŸ”](Topic_Google.md), [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [AI for images ğŸ–¼ï¸](Topic_AI_for_images.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-05-21 |
| [Hugging Face is sharing $10 million worth of compute to help beat the big AI companies](https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai) ğŸŸ¢ | Hugging Face is dedicating $10M in free GPU resources to support AI developers, startups, and academics. Their ZeroGPU initiative, part of Hugging Face Spaces, offers communal GPU access, aiming to reduce computational access barriers and improve cost-efficiency. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-05-21 |
| [Googleâ€™s new chips look to challenge Nvidia, Microsoft and Amazon](https://qz.com/google-ai-chip-nvidia-axion-arm-microsoft-1851397201) ğŸŸ¢ | Google has unveiled the Cloud TPU v5p, an AI chip that delivers nearly triple the training speed of its predecessor, the TPU v4, reinforcing its position in AI services and hardware. At the Google Cloud Next event, CEO Pichai highlighted the companyâ€™s AI advancements and collaborations, including the use of the A3 supercomputer and Blackwell chips in the AI Hypercomputer. Additionally, Google introduced the Google Axion CPU, an Arm-based processor that competes with similar offerings from Microsoft and Amazon, boasting a 30% performance improvement and better energy efficiency. | [Amazon ğŸŒ³](Topic_Amazon.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Google ğŸ”](Topic_Google.md), [Microsoft ğŸªŸ](Topic_Microsoft.md) | 2024-04-23 |
| [Lambda Announces $500M GPU-Backed Facility to Expand Cloud for AI](https://www.businesswire.com/news/home/20240402148086/en/Lambda-Announces-500M-GPU-Backed-Facility-to-Expand-Cloud-for-AI) ğŸŸ¢ | Lambda has successfully secured $500 million in funding to enhance its AI-oriented cloud services, powered by NVIDIA GPUs, following a Series C investment round. | [Funding ğŸ’°](Topic_Funding.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-04-08 |
| [OpenAI and Microsoft reportedly planning $100 billion datacenter project for an AI supercomputer](https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer) ğŸŸ¢ | Microsoft and OpenAI have announced a partnership to construct â€œStargate,â€ an advanced AI supercomputer in the U.S., featuring millions of GPUs. The project, which may exceed $115 billion, represents a major commitment to expanding datacenter capabilities to advance AI research and development. | [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-04-02 |
| [Amazon and Anthropic deepen their shared commitment to advancing generative AI](https://www.aboutamazon.com/news/company-news/amazon-anthropic-ai-investment) ğŸŸ¢ | Amazon has invested $4 billion in AI company Anthropic to further develop AI technologies. Anthropic leverages Amazon Web Services (AWS) Trainium and Inferentia chips for enhancing their AI models. Notably, Anthropicâ€™s Claude 3 models have been incorporated into Amazon Bedrock by AWS. | [Amazon ğŸŒ³](Topic_Amazon.md), [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Claude ğŸ–‹ï¸](Topic_Claude.md), [Anthropic ğŸŒ](Topic_Anthropic.md) | 2024-04-02 |
| [â€˜We Created a Processor for the Generative AI Era,â€™ NVIDIA CEO Says](https://blogs.nvidia.com/blog/2024-gtc-keynote/) ğŸŸ¢ | NVIDIA CEO Jensen Huang announced the NVIDIA Blackwell computing platform at the GTC conference, aimed at advancing generative AI with superior training and inference capabilities. The platform includes enhanced interconnects for better performance and scalability. NVIDIA also launched NIM microservices for tailored AI deployment and Omniverse Cloud APIs for sophisticated simulation, signaling a transformative impact on sectors like healthcare and robotics. | [AI in healthcare ğŸ¥](Topic_AI_in_healthcare.md), [Robotics ğŸ¤–](Topic_Robotics.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-03-25 |
| [Nvidiaâ€™s data center GPU sales grow by a stunning 409% on huge demand for AI chips](https://siliconangle.com/2024/02/21/nvidias-data-center-gpu-sales-grow-stunning-409-huge-demand-ai-chips/) ğŸŸ¢ | Nvidia has experienced a significant surge in GPU sales, reporting a 409% increase due in large part to the rising demand for AI technologies. With Q4 earnings and revenue significantly outpacing Wall Street forecasts, the companyâ€™s financials have thrived on the back of robust sales from their Hopper GPU series, including the H100. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-02-26 |
| [GPU cloud company Together AI to raise $100m](https://www.datacenterdynamics.com/en/news/together-ai-set-to-receive-100m-in-funding-round-led-by-salesforce-ventures/) ğŸŸ¢ | Together AI, a GPU cloud company specializing in open-source AI tools and Nvidia server chip access, is nearing a $100 million funding round led by Salesforce Ventures, potentially elevating its valuation to $1 billion. | [Funding ğŸ’°](Topic_Funding.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-02-19 |
| [OpenAIâ€™s CEO Sam Altman is chasing trillions of dollars as investments to disrupt AI, chip industries](https://www.firstpost.com/tech/openai-sam-altman-is-chasing-trillions-of-dollars-as-investments-to-disrupt-ai-chip-industries-13708732.html) ğŸŸ¢ | Sam Altman, CEO of OpenAI, is actively seeking to secure $5â€“7 trillion in funding to expand the semiconductor industry to support AI development. This investment aims to address GPU shortages and foster the growth of both AI and artificial general intelligence. Altman is engaging with various stakeholders, including government officials from the UAE and the US, investors, and chip manufacturers, in his efforts to build a robust global chip-making infrastructure that meets the increasing demands and energy requirements of AI facilities. | [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-02-12 |
| [OpenAIâ€™s Sam Altman Is Raising Money to Set Up AI Chip Factories](https://beebom.com/openai-sam-altman-raising-money-ai-chip-factories/) ğŸŸ¢ | OpenAI CEO Sam Altman is actively seeking investment, potentially over $8 billion, from entities including G42 and SoftBank to establish AI chip factories aimed at meeting the surging demand for specialized processors in the growing AI industry. | [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2024-01-29 |
| [Hugging Face and Google partner for open AI collaboration](https://huggingface.co/blog/gcp-partnership) ğŸŸ¢ | Hugging Face has partnered with Google Cloud, providing users with access to enhanced AI models and integration with Google Cloud services like GKE and Vertex AI, utilizing Google TPUs and NVIDIA H100 GPUs. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Google ğŸ”](Topic_Google.md) | 2024-01-29 |
| [Mark Zuckerberg indicates Meta is spending billions of dollars on Nvidia AI chips](https://www.cnbc.com/2024/01/18/mark-zuckerberg-indicates-meta-is-spending-billions-on-nvidia-ai-chips.html) ğŸŸ¢ | Meta plans a significant investment in AI research by integrating 350,000 Nvidia H100 GPUs by 2024. Given their high cost â€” estimated between $25K-$30K â€” this investment underlines Metaâ€™s commitment to scaling up computing power. Overall, Metaâ€™s strategy to amass the computational equivalent of 600K H100 GPUs highlights a substantial push to enhance its AI capabilities. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Meta â™¾](Topic_Meta.md) | 2024-01-22 |
| [Cristiano Amon: generative AI is â€˜evolving very, very fastâ€™ into mobile devices](https://www.ft.com/content/dbc0984b-4801-4aeb-bcab-480704c34161) ğŸŸ¢ | Qualcomm CEO Cristiano Amon envisions generative AI rapidly integrating into mobiles, PCs, and cars, aiming to offer enriched user experiences by complementing cloud AI. Leveraging Qualcommâ€™s efficient AI processors, these advancements are set to facilitate real-time AI applications on battery- operated devices, proactively meeting user needs. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2024-01-08 |
| [Introducing gigaGPT: GPT-3 sized models in 565 lines of code](https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code) ğŸŸ¢ | Cerebras has released gigaGPT, a model implementation similar to nanoGPT but with over 100 billion parameters. By leveraging Cerebras hardware and different optimizers, gigaGPT overcomes the limitations of GPU memory and the need for complex scaling frameworks, offering a simplified approach for training large models. | [Model release ğŸ‰](Topic_Model_release.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-19 |
| [Nvidia unveils H200, its newest high-end chip for training AI models](https://www.cnbc.com/2023/11/13/nvidia-unveils-h200-its-newest-high-end-chip-for-training-ai-models.html) ğŸŸ¢ | Nvidia introduces the H200 GPU, an upgraded version with 141GB of high-bandwidth memory. This enhancement helps with the inference process in training large AI models. Set to be released in Q2 2024, the H200 will compete with AMDâ€™s MI300X GPU, which also offers increased memory capacity for handling big models. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-11-20 |
| [NVIDIA makes Pandas much faster leveraging GPUs](https://rapids.ai/cudf-pandas/) ğŸŸ¢ | NVIDIA has significantly enhanced the Pandas library, achieving up to 150 times faster performance by capitalizing on GPUs. With the new cudf.pandas module, operations are seamlessly executed on either the GPU or CPU, providing automatic synchronization and efficient switching between the two. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-11-13 |
| [New AWS service lets customers rent Nvidia GPUs for quick AI projects](https://techcrunch.com/2023/11/01/new-aws-service-lets-customers-rent-nvidia-gpus-for-quick-ai-projects/) ğŸŸ¢ | AWS has introduced Amazon Elastic Compute Cloud (EC2) Capacity Blocks for ML, enabling AI practitioners to reserve Nvidia GPUs for specific time periods. This new service grants access to Nvidia H100 Tensor Core GPU instances, allowing users to reserve instances for up to 14 days ahead of time, with shutdowns scheduled automatically. | [Amazon ğŸŒ³](Topic_Amazon.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-11-13 |
| [New AWS service lets customers rent Nvidia GPUs for quick AI projects](https://techcrunch.com/2023/11/01/new-aws-service-lets-customers-rent-nvidia-gpus-for-quick-ai-projects/) ğŸŸ¢ | AWS launches Amazon Elastic Compute Cloud (EC2) Capacity Blocks for ML, allowing customers to rent Nvidia GPUs for specific time frames. | [Amazon ğŸŒ³](Topic_Amazon.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-11-06 |
| [Microsoft to Unveil In-House AI Chip, Reducing Reliance on NVIDIA](https://www.maginative.com/article/microsoft-to-unveil-in-house-ai-chip-reducing-reliance-on-nvidia/) ğŸ”´ | Microsoft is soon launching its own AI chip called Athena, aiming to reduce reliance on NVIDIA GPUs and compete against NVIDIAâ€™s H100 GPU for AI acceleration in data centers. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md) | 2023-10-10 |
| [OpenAI is exploring making its own AI chips](https://www.businessinsider.com/openai-is-considering-making-its-own-ai-chips-chatgpt-2023-10) ğŸŸ¢ | OpenAI is considering developing its own AI chips for ChatGPT due to a global shortage of processors for training AI models. This move could help reduce the high operating costs of ChatGPT, which currently amount to $700,000 per day. OpenAIâ€™s decision may diverge from Microsoft, their partner, who is also working on their own AI chips. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-10-10 |
| [Introducing Stable LM 3B: Bringing Sustainable, High-Performance Language Models to Smart Devices](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices) ğŸŸ¢ | Stability AI introduces Stable LM 3B, a high-performing language model designed for smart devices. With 3 billion parameters, it outperforms state-of-the-art 3B models and reduces operating costs and power consumption. The model enables a wider range of applications on smart devices, PCs, and edge computing. | [Stability AI âš–ï¸](Topic_Stability_AI.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-10-10 |
| [Amazon will invest up to $4 billion in Anthropic](https://www.anthropic.com/index/anthropic-amazon) ğŸŸ¢ | Amazon has made a significant $4 billion investment in Anthropic. This partnership will enable Anthropic to benefit from Amazon Web Services (AWS), specifically leveraging AWSâ€™s Trainium and Inferentia chips to enhance model training and deployment capabilities. Additionally, Anthropic will use Amazon Bedrock to optimize Claude versions and explore finetuning options. | [Amazon ğŸŒ³](Topic_Amazon.md), [Funding ğŸ’°](Topic_Funding.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Claude ğŸ–‹ï¸](Topic_Claude.md), [Anthropic ğŸŒ](Topic_Anthropic.md) | 2023-10-02 |
| [LLM Startup Embraces AMD GPUs, Says ROCm Has â€˜Parityâ€™ With Nvidiaâ€™s CUDA Platform](https://www.crn.com/news/components-peripherals/llm-startup-embraces-amd-gpus-says-rocm-has-parity-with-nvidia-s-cuda-platform) ğŸŸ¢ | A startup called Lamini is using over 100 AMD Instinct MI200 GPUs and found that AMDâ€™s ROCm software platform rivals Nvidiaâ€™s CUDA platform. They claim that running a large language model on their platform is 10x cheaper than on Amazon Web Services. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [NVIDIA ğŸ®](Topic_NVIDIA.md) | 2023-10-02 |
| [NVIDIA and Hugging Face to Connect Millions of Developers to Generative AI Supercomputing](https://nvidianews.nvidia.com/news/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing) ğŸŸ¢ | NVIDIA and Hugging Face have partnered to offer AI developers access to high-performance GPUs for deep learning through DGX Cloud integration. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-08-14 |
| [Cerebras Systems signs $100 million AI supercomputer deal with UAEâ€™s G42](https://www.reuters.com/technology/cerebras-systems-signs-100-mln-ai-supercomputer-deal-with-uaes-g42-2023-07-20/) ğŸŸ¢ | Cerebras Systems has struck a $100 million deal with G42, marking the debut of AI supercomputers that could potentially challenge Nvidiaâ€™s market position. In response to chip shortages, cloud computing providers are seeking alternative solutions. To accelerate the rollout, Cerebras will construct three Condor Galaxy systems in the United States, with the first supercomputer set to go online this year, followed by two others in early 2024. | [Funding ğŸ’°](Topic_Funding.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-07-24 |
| [Nvidia deepens bets on AI in drug discovery with Recursion investment](https://www.reuters.com/technology/nvidia-invests-50-mln-recursion-train-ai-models-drug-discovery-2023-07-12/) ğŸŸ¢ | Nvidia invests $50M in Recursion, a biotech company using AI to revolutionize drug discovery. This partnership allows Recursion to utilize Nvidiaâ€™s platform and access their advanced AI technology. Recursionâ€™s share price surged by 83% post-announcement, highlighting the marketâ€™s recognition of AIâ€™s significance in drug discovery. | [AI in healthcare ğŸ¥](Topic_AI_in_healthcare.md), [Funding ğŸ’°](Topic_Funding.md), [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-07-17 |
| [Miner Pivots 38,000 GPUs From Crypto to AI](https://www.tomshardware.com/news/hive-blockchain-pivoting-to-ai) ğŸŸ¢ | Cryptomining firm Hive Blockchain is redirecting their efforts from Ethereum mining to AI workloads. With 38,000 GPUs at their disposal, they aim to generate revenue while still utilizing some GPU power for crypto mining. However, transitioning to AI compute poses challenges as older ETH mining GPUs have limited value in this market. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-07-10 |
| [Inside Chinaâ€™s underground market for high-end Nvidia AI chips](https://www.reuters.com/technology/inside-chinas-underground-market-high-end-nvidia-ai-chips-2023-06-19/) ğŸ”´ | Despite U.S. export restrictions, Chinese demand for high-end Nvidia AI chips remains strong and vendors in Shenzhen and Hong Kong have emerged to offer them at steep prices in a de facto underground market. The exact volume of chip flow is unknown, but local authorities are also reported to be buyers. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [AI regulation ğŸ“œ](Topic_AI_regulation.md) | 2023-06-26 |
| [Chinaâ€™s ByteDance Has Gobbled Up $1 Billion of Nvidia GPUs for AI This Year](https://www.tomshardware.com/news/chinas-bytedance-has-gobbled-up-dollar1-billion-of-nvidia-gpus-for-ai-this-year) ğŸŸ¢ | Chinese tech giant ByteDance has reportedly invested $1 billion in Nvidiaâ€™s HPC products, including the A100 and H800 cards, totaling about 100,000 units, to fulfill the high demand for AI technology. China recognizes the impact of AI on the global market and has embraced it, investing heavily in HPC hardware. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-06-26 |
| [OpenAIâ€™s plans according to Sam Altman](https://website-754fwhahs-humanloopml.vercel.app/blog/open_ai_talk) ğŸŸ¢ | OpenAI plans to prioritize creating a cheaper and faster GPT-4, extending context windows, and releasing multimodality in 2024. However, they are currently limited by GPU shortages and are unable to make models millions of times bigger in the near future. The finetuning API will be extended to the latest models. | [Multimodal AI (image, video, audio) ğŸ“¸](Topic_Multimodal_AI_(image_video_audio).md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [GPT-4 and GPT-4 turbo ğŸš€](Topic_GPT-4_and_GPT-4_turbo.md), [OpenAI ğŸŒŸ](Topic_OpenAI.md) | 2023-06-06 |
| [Why Nvidia is suddenly one of the most valuable companies in the world](https://www.washingtonpost.com/technology/2023/05/25/nvidia-ai-stock-gpu-chatbots/) ğŸŸ¢ | NVIDIAâ€™s GPUs have become a crucial component in developing AI, leading its worth to $939.3 billion; with AI applications requiring huge amounts of data, companies are buying thousands of NVIDIAâ€™s expensive chips. NVIDIAâ€™s dominance in the industry is predicted to persist as startups compete with big tech for access to its costly GPUs. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-06-06 |
| [Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters/) ğŸŸ¢ | Intel has announced the Aurora genAI model, with 1 trillion parameters, which will be trained on scientific texts and structured scientific data to target cancer research, systems biology, cosmology, polymer chemistry, materials, and climate science. It will be powered by the Aurora supercomputer and has potential to suggest experiments and accelerate drug design targets. | [AI in healthcare ğŸ¥](Topic_AI_in_healthcare.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-05-29 |
| [Meta bets big on AI with custom chips and a supercomputer](https://techcrunch.com/2023/05/18/meta-bets-big-on-ai-with-custom-chips-and-a-supercomputer/) ğŸŸ¢ | Meta lifted the curtains on its efforts to develop in-house infrastructure for AI workloads, including generative AI. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Meta â™¾](Topic_Meta.md) | 2023-05-22 |
| [Microsoft Readies AI Chip as Machine Learning Costs Surge](https://www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge) ğŸŸ¢ | Microsoft creating Athena chip for AIâ€™s large-language models, aiming to save money and time. Amazon, Google & Facebook are creating their own AI chips too. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md) | 2023-04-24 |
| [Elon Musk is moving forward with a new generative-AI project at Twitter after purchasing thousands of GPUs](https://www.businessinsider.com/elon-musk-twitter-investment-generative-ai-project-2023-4) ğŸŸ¢ | Elon Musk is pursuing a Twitter AI project with a large language model despite recently calling to halt AI training. | [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md) | 2023-04-17 |
| [Microsoft spent hundreds of millions of dollars on a ChatGPT supercomputer](https://www.theverge.com/2023/3/13/23637675/microsoft-chatgpt-bing-millions-dollars-supercomputer-openai) ğŸŸ¢ | Microsoft says it connected tens of thousands of Nvidia A100 chips and reworked server racks to build the hardware behind ChatGPT and its own Bing AI bot. | [NVIDIA ğŸ®](Topic_NVIDIA.md), [AI Chips and GPUs ğŸ–¥ï¸](Topic_AI_Chips_and_GPUs.md), [Microsoft ğŸªŸ](Topic_Microsoft.md), [ChatGPT ğŸ’¬](Topic_ChatGPT.md) | 2023-03-20 |