# Mixture-of-Experts news

| Title | Summary | Topics | Week |
| --- | --- | --- | --- |
| [Snowflake releases Arctic, an open LLM for Enterprise AI](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/) ğŸŸ¢ | Snowflake AI Research has released Arctic, a cost-effective enterprise AI LLM featuring a Dense-MoE Hybrid transformer architecture with 480 billion parameters. Trained for less than $2 million, Arctic excels in tasks like SQL generation and coding. Itâ€™s fully open-source under Apache 2.0, providing free access to model weights and code. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Funding ğŸ’°](Topic_Funding.md) | 2024-04-29 |
| [Mistral released Mixtral 8x22B](https://mistral.ai/news/mixtral-8x22b/) ğŸŸ¢ | Mistral unveiled Mixtral 8x22B, an efficient sparse Mixture-of-Experts model with 39B active out of 141B total parameters, specializing in multilingual communication, coding, mathematics, and excelling in reasoning and knowledge tasks. The model boasts a 64K token context window, is compatible with multiple platforms, and is available under the open-source Apache 2.0 license. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md) | 2024-04-23 |
| [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) ğŸŸ¢ | Databricks has unveiled DBRX, a new open-source large language model (LLM) that surpasses GPT-3.5 in programming and general tasks, and is competitive with Gemini 1.0 Pro. DBRX features a mixture-of-experts architecture with a massive 132 billion parameters, though only 36 billion are active for any given input. Both DBRX Base and DBRX Instruct models are openly accessible on Hugging Face. | [Hugging Face ğŸ¤—](Topic_Hugging_Face.md), [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [AI for coding ğŸ‘¨â€ğŸ’»](Topic_AI_for_coding.md), [Model release ğŸ‰](Topic_Model_release.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2024-04-02 |
| [Grok open release](https://github.com/xai-org/grok-1) ğŸŸ¢ | xAI has released Grok-1, a language Mixture-of-Experts model with 314 billion parameters, following its pre-training in October 2023. This base model checkpoint is intended for further research and the development of conversational applications, and it is accessible under the Apache 2.0 license. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [Grok ğŸ¦](Topic_Grok.md), [Model release ğŸ‰](Topic_Model_release.md) | 2024-03-18 |
| [Mistral AI has released Mixtral 8x7B, an open-source sparse mixture-of-experts model](https://mistral.ai/news/mixtral-of-experts/) ğŸŸ¢ | Mistral AI has introduced Mixtral 8x7B, an open-source, sparse mixture-of-experts model with 45B parameters. It outperforms LLaMA 2 70B and GPT-3.5 in NLP benchmarks, while also providing 6x faster inference speed. The model selectively uses only 12B parameters per inference, keeping operational costs equivalent to a 12B model. The Mixtral model is accessible through Mistralâ€™s beta API and dev platform. | [Mixture-of-Experts ğŸ¤](Topic_Mixture-of-Experts.md), [Model release ğŸ‰](Topic_Model_release.md), [Mistral ğŸŒ¬ï¸](Topic_Mistral.md), [GPT-3, GPT-3.5, and GPT-3.5 turbo ğŸ’¡](Topic_GPT-3_GPT-3.5_and_GPT-3.5_turbo.md) | 2023-12-11 |